{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Helpo! Helpo!! A place for aws & general python helpers Documentation available at: https://meatware.github.io/helpohelpo/index.html Run ./deploy_pdoc_documentation.sh to update documentation with pdoc Documentation generated by portray","title":"Home"},{"location":"#helpo-helpo","text":"","title":"Helpo! Helpo!!"},{"location":"#a-place-for-aws-general-python-helpers","text":"","title":"A place for aws &amp; general python helpers"},{"location":"#documentation-available-at","text":"https://meatware.github.io/helpohelpo/index.html Run ./deploy_pdoc_documentation.sh to update documentation with pdoc Documentation generated by portray","title":"Documentation available at:"},{"location":"reference/helpers/","text":"Module helpers The init.py. View Source \"\"\"The init.py.\"\"\" Sub-modules helpers.aws_general helpers.check_env_vars helpers.data_structure_search helpers.date_tools helpers.dateframe_tools helpers.dynamic_loader helpers.file_tools helpers.os_functions helpers.plotter_tools helpers.string_tools","title":"Index"},{"location":"reference/helpers/#module-helpers","text":"The init.py. View Source \"\"\"The init.py.\"\"\"","title":"Module helpers"},{"location":"reference/helpers/#sub-modules","text":"helpers.aws_general helpers.check_env_vars helpers.data_structure_search helpers.date_tools helpers.dateframe_tools helpers.dynamic_loader helpers.file_tools helpers.os_functions helpers.plotter_tools helpers.string_tools","title":"Sub-modules"},{"location":"reference/helpers/aws_general/","text":"Module helpers.aws_general General Boto3 convenience module. View Source \"\"\"General Boto3 convenience module.\"\"\" import sys import logging import boto3 import botocore.exceptions from botocore.client import ClientError LOG = logging . getLogger ( __name__ ) def boto_session ( region ): \"\"\"Initialise boto3 session.\"\"\" session = boto3 . Session ( region_name = region ) return session def _add_ec2_tags ( region , ec2_list ): \"\"\"Add schedule_deletion tag to ec2 instances.\"\"\" ec2_client = boto3 . client ( \"ec2\" , region_name = region ) if len ( ec2_list ) > 0 : ec2_tag_status = { \"ec2_tag_statuses\" : []} aws_sd_tag_val = helpo . calc_ndays_fwd ( ndays = 5 ) for ec2_instance in ec2_list : del_tag_status = try_except_status ( partial ( ec2_client . create_tags , Resources = [ ec2_instance ], Tags = [{ \"Key\" : \"schedule_deletion\" , \"Value\" : aws_sd_tag_val }], ), \"Error in del_tag_status: %s \" , ) ec2_tag_status [ \"ec2_tag_statuses\" ] . append ( { \"ec2_instance\" : ec2_instance , \"del_tag_status\" : del_tag_status } ) return ec2_tag_status return {} def search_ec2_instances ( ec2_client , filter_data ): \"\"\"search aws region for ec2 instances to delete.\"\"\" try : response = ec2_client . describe_instances ( Filters = filter_data ) return response except ClientError as err : LOG . warning ( \" %s \" , str ( err )) return False def terminate_instance ( instance_id , region_name ): \"\"\"Terminates ec2 instance by id.\"\"\" ec2 = boto3 . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ) . terminate () except ClientError as err : LOG . warning ( \" %s \" , str ( err )) if err . response [ \"Error\" ][ \"Code\" ] == \"OperationNotPermitted\" : return \"<!here|here> Warning! Modify termination protection: \" + instance_id else : return \"Uncaught error occured on termination: \" + instance_id return \"Successfully terminated\" def shutdown_instance ( instance_id , region_name ): \"\"\"Shutdown ec2 instance by id.\"\"\" bo3_int = boto3 . Session ( region_name = region_name ) ec2 = bo3_int . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ) . stop () except ClientError as err : LOG . warning ( \"Shutdown Error: %s \" , str ( err )) return \"Uncaught error occured on shut down: \" + instance_id return \"Successfully shutdown\" def check_bucket_exists ( bucket_name ): \"\"\"Sanity check whether s3 bucket exists.\"\"\" s3_client = boto3 . resource ( \"s3\" ) try : s3_client . meta . client . head_bucket ( Bucket = bucket_name ) except ClientError : LOG . critical ( \"s3 bucket %s does not exist or access denied\" , bucket_name ) sys . exit ( 0 ) def upload_logs_s3 ( bucket_name , log_name ): \"\"\"Upload log data to s3 bucket.\"\"\" data = open ( log_name , \"rb\" ) s3_client = boto3 . resource ( \"s3\" ) try : s3_client . Bucket ( bucket_name ) . put_object ( Key = log_name , Body = data ) LOG . info ( \"Uploading log file %s to s3 %s \" , log_name , bucket_name ) except Exception as err : LOG . warning ( \"Failed to Upload log file %s to s3 %s \" , log_name , bucket_name ) LOG . critical ( \"error: %s \" , err ) return True def tagapi_search_tags ( client , token ): try : response = client . get_resources ( PaginationToken = token , TagFilters = [{ \"Key\" : \"schedule_deletion\" }], ResourcesPerPage = 50 , ResourceTypeFilters = [ \"elasticloadbalancing:loadbalancer\" ,], ) return response except ClientError as err : LOG . warning ( \" %s \" , str ( err )) def convert_http_status ( status ): \"\"\"Converts HTTP 200 to OK or dispays error message from def try_except_status. \"\"\" if status == 200 : return \": *OK*\" return \": *FAIL - \" + status + \"*\" def try_except_status ( bo3_client_method , fail_str ): \"\"\"Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way \"\"\" try : get_status = bo3_client_method status = get_status ()[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] except ClientError as err : LOG . warning ( fail_str , str ( err )) if err . response [ \"Error\" ][ \"Code\" ]: status = err . response [ \"Error\" ][ \"Code\" ] else : status = str ( err ) return status Variables LOG Functions boto_session def boto_session ( region ) Initialise boto3 session. View Source def boto_session ( region ): \"\"\"Initialise boto3 session.\"\"\" session = boto3 . Session ( region_name = region ) return session check_bucket_exists def check_bucket_exists ( bucket_name ) Sanity check whether s3 bucket exists. View Source def check_bucket_exists ( bucket_name ): \"\"\"Sanity check whether s3 bucket exists.\"\"\" s3_client = boto3 . resource ( \"s3\" ) try : s3_client . meta . client . head_bucket ( Bucket = bucket_name ) except ClientError : LOG . critical ( \"s3 bucket %s does not exist or access denied\" , bucket_name ) sys . exit ( 0 ) convert_http_status def convert_http_status ( status ) Converts HTTP 200 to OK or dispays error message from def try_except_status. View Source def convert_http_status ( status ): \"\"\"Converts HTTP 200 to OK or dispays error message from def try_except_status. \"\"\" if status == 200 : return \": *OK*\" return \": *FAIL - \" + status + \"*\" search_ec2_instances def search_ec2_instances ( ec2_client , filter_data ) search aws region for ec2 instances to delete. View Source def search_ec2_instances ( ec2_client , filter_data ): \"\"\"search aws region for ec2 instances to delete.\"\"\" try : response = ec2_client . describe_instances ( Filters = filter_data ) return response except ClientError as err : LOG . warning ( \"%s\" , str ( err )) return False shutdown_instance def shutdown_instance ( instance_id , region_name ) Shutdown ec2 instance by id. View Source def shutdown_instance ( instance_id , region_name ): \"\"\"Shutdown ec2 instance by id.\"\"\" bo3_int = boto3 . Session ( region_name = region_name ) ec2 = bo3_int . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ). stop () except ClientError as err : LOG . warning ( \"Shutdown Error: %s\" , str ( err )) return \"Uncaught error occured on shut down: \" + instance_id return \"Successfully shutdown\" tagapi_search_tags def tagapi_search_tags ( client , token ) View Source def tagapi_search_tags ( client , token ): try : response = client . get_resources ( PaginationToken = token , TagFilters = [ { \"Key\" : \"schedule_deletion\" } ], ResourcesPerPage = 50 , ResourceTypeFilters = [ \"elasticloadbalancing:loadbalancer\" ,], ) return response except ClientError as err : LOG . warning ( \"%s\" , str ( err )) terminate_instance def terminate_instance ( instance_id , region_name ) Terminates ec2 instance by id. View Source def terminate_instance ( instance_id , region_name ): \"\"\"Terminates ec2 instance by id.\"\"\" ec2 = boto3 . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ). terminate () except ClientError as err : LOG . warning ( \"%s\" , str ( err )) if err . response [ \"Error\" ][ \"Code\" ] == \"OperationNotPermitted\" : return \"<!here|here> Warning! Modify termination protection: \" + instance_id else : return \"Uncaught error occured on termination: \" + instance_id return \"Successfully terminated\" try_except_status def try_except_status ( bo3_client_method , fail_str ) Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way View Source def try_except_status ( bo3_client_method , fail_str ): \"\"\"Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way \"\"\" try : get_status = bo3_client_method status = get_status ()[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] except ClientError as err : LOG . warning ( fail_str , str ( err )) if err . response [ \"Error\" ][ \"Code\" ]: status = err . response [ \"Error\" ][ \"Code\" ] else : status = str ( err ) return status upload_logs_s3 def upload_logs_s3 ( bucket_name , log_name ) Upload log data to s3 bucket. View Source def upload_logs_s3 ( bucket_name , log_name ): \"\"\"Upload log data to s3 bucket.\"\"\" data = open ( log_name , \"rb\" ) s3_client = boto3 . resource ( \"s3\" ) try : s3_client . Bucket ( bucket_name ). put_object ( Key = log_name , Body = data ) LOG . info ( \"Uploading log file %s to s3 %s\" , log_name , bucket_name ) except Exception as err : LOG . warning ( \"Failed to Upload log file %s to s3 %s\" , log_name , bucket_name ) LOG . critical ( \"error: %s\" , err ) return True","title":"Aws General"},{"location":"reference/helpers/aws_general/#module-helpersaws_general","text":"General Boto3 convenience module. View Source \"\"\"General Boto3 convenience module.\"\"\" import sys import logging import boto3 import botocore.exceptions from botocore.client import ClientError LOG = logging . getLogger ( __name__ ) def boto_session ( region ): \"\"\"Initialise boto3 session.\"\"\" session = boto3 . Session ( region_name = region ) return session def _add_ec2_tags ( region , ec2_list ): \"\"\"Add schedule_deletion tag to ec2 instances.\"\"\" ec2_client = boto3 . client ( \"ec2\" , region_name = region ) if len ( ec2_list ) > 0 : ec2_tag_status = { \"ec2_tag_statuses\" : []} aws_sd_tag_val = helpo . calc_ndays_fwd ( ndays = 5 ) for ec2_instance in ec2_list : del_tag_status = try_except_status ( partial ( ec2_client . create_tags , Resources = [ ec2_instance ], Tags = [{ \"Key\" : \"schedule_deletion\" , \"Value\" : aws_sd_tag_val }], ), \"Error in del_tag_status: %s \" , ) ec2_tag_status [ \"ec2_tag_statuses\" ] . append ( { \"ec2_instance\" : ec2_instance , \"del_tag_status\" : del_tag_status } ) return ec2_tag_status return {} def search_ec2_instances ( ec2_client , filter_data ): \"\"\"search aws region for ec2 instances to delete.\"\"\" try : response = ec2_client . describe_instances ( Filters = filter_data ) return response except ClientError as err : LOG . warning ( \" %s \" , str ( err )) return False def terminate_instance ( instance_id , region_name ): \"\"\"Terminates ec2 instance by id.\"\"\" ec2 = boto3 . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ) . terminate () except ClientError as err : LOG . warning ( \" %s \" , str ( err )) if err . response [ \"Error\" ][ \"Code\" ] == \"OperationNotPermitted\" : return \"<!here|here> Warning! Modify termination protection: \" + instance_id else : return \"Uncaught error occured on termination: \" + instance_id return \"Successfully terminated\" def shutdown_instance ( instance_id , region_name ): \"\"\"Shutdown ec2 instance by id.\"\"\" bo3_int = boto3 . Session ( region_name = region_name ) ec2 = bo3_int . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ) . stop () except ClientError as err : LOG . warning ( \"Shutdown Error: %s \" , str ( err )) return \"Uncaught error occured on shut down: \" + instance_id return \"Successfully shutdown\" def check_bucket_exists ( bucket_name ): \"\"\"Sanity check whether s3 bucket exists.\"\"\" s3_client = boto3 . resource ( \"s3\" ) try : s3_client . meta . client . head_bucket ( Bucket = bucket_name ) except ClientError : LOG . critical ( \"s3 bucket %s does not exist or access denied\" , bucket_name ) sys . exit ( 0 ) def upload_logs_s3 ( bucket_name , log_name ): \"\"\"Upload log data to s3 bucket.\"\"\" data = open ( log_name , \"rb\" ) s3_client = boto3 . resource ( \"s3\" ) try : s3_client . Bucket ( bucket_name ) . put_object ( Key = log_name , Body = data ) LOG . info ( \"Uploading log file %s to s3 %s \" , log_name , bucket_name ) except Exception as err : LOG . warning ( \"Failed to Upload log file %s to s3 %s \" , log_name , bucket_name ) LOG . critical ( \"error: %s \" , err ) return True def tagapi_search_tags ( client , token ): try : response = client . get_resources ( PaginationToken = token , TagFilters = [{ \"Key\" : \"schedule_deletion\" }], ResourcesPerPage = 50 , ResourceTypeFilters = [ \"elasticloadbalancing:loadbalancer\" ,], ) return response except ClientError as err : LOG . warning ( \" %s \" , str ( err )) def convert_http_status ( status ): \"\"\"Converts HTTP 200 to OK or dispays error message from def try_except_status. \"\"\" if status == 200 : return \": *OK*\" return \": *FAIL - \" + status + \"*\" def try_except_status ( bo3_client_method , fail_str ): \"\"\"Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way \"\"\" try : get_status = bo3_client_method status = get_status ()[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] except ClientError as err : LOG . warning ( fail_str , str ( err )) if err . response [ \"Error\" ][ \"Code\" ]: status = err . response [ \"Error\" ][ \"Code\" ] else : status = str ( err ) return status","title":"Module helpers.aws_general"},{"location":"reference/helpers/aws_general/#variables","text":"LOG","title":"Variables"},{"location":"reference/helpers/aws_general/#functions","text":"","title":"Functions"},{"location":"reference/helpers/aws_general/#boto_session","text":"def boto_session ( region ) Initialise boto3 session. View Source def boto_session ( region ): \"\"\"Initialise boto3 session.\"\"\" session = boto3 . Session ( region_name = region ) return session","title":"boto_session"},{"location":"reference/helpers/aws_general/#check_bucket_exists","text":"def check_bucket_exists ( bucket_name ) Sanity check whether s3 bucket exists. View Source def check_bucket_exists ( bucket_name ): \"\"\"Sanity check whether s3 bucket exists.\"\"\" s3_client = boto3 . resource ( \"s3\" ) try : s3_client . meta . client . head_bucket ( Bucket = bucket_name ) except ClientError : LOG . critical ( \"s3 bucket %s does not exist or access denied\" , bucket_name ) sys . exit ( 0 )","title":"check_bucket_exists"},{"location":"reference/helpers/aws_general/#convert_http_status","text":"def convert_http_status ( status ) Converts HTTP 200 to OK or dispays error message from def try_except_status. View Source def convert_http_status ( status ): \"\"\"Converts HTTP 200 to OK or dispays error message from def try_except_status. \"\"\" if status == 200 : return \": *OK*\" return \": *FAIL - \" + status + \"*\"","title":"convert_http_status"},{"location":"reference/helpers/aws_general/#search_ec2_instances","text":"def search_ec2_instances ( ec2_client , filter_data ) search aws region for ec2 instances to delete. View Source def search_ec2_instances ( ec2_client , filter_data ): \"\"\"search aws region for ec2 instances to delete.\"\"\" try : response = ec2_client . describe_instances ( Filters = filter_data ) return response except ClientError as err : LOG . warning ( \"%s\" , str ( err )) return False","title":"search_ec2_instances"},{"location":"reference/helpers/aws_general/#shutdown_instance","text":"def shutdown_instance ( instance_id , region_name ) Shutdown ec2 instance by id. View Source def shutdown_instance ( instance_id , region_name ): \"\"\"Shutdown ec2 instance by id.\"\"\" bo3_int = boto3 . Session ( region_name = region_name ) ec2 = bo3_int . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ). stop () except ClientError as err : LOG . warning ( \"Shutdown Error: %s\" , str ( err )) return \"Uncaught error occured on shut down: \" + instance_id return \"Successfully shutdown\"","title":"shutdown_instance"},{"location":"reference/helpers/aws_general/#tagapi_search_tags","text":"def tagapi_search_tags ( client , token ) View Source def tagapi_search_tags ( client , token ): try : response = client . get_resources ( PaginationToken = token , TagFilters = [ { \"Key\" : \"schedule_deletion\" } ], ResourcesPerPage = 50 , ResourceTypeFilters = [ \"elasticloadbalancing:loadbalancer\" ,], ) return response except ClientError as err : LOG . warning ( \"%s\" , str ( err ))","title":"tagapi_search_tags"},{"location":"reference/helpers/aws_general/#terminate_instance","text":"def terminate_instance ( instance_id , region_name ) Terminates ec2 instance by id. View Source def terminate_instance ( instance_id , region_name ): \"\"\"Terminates ec2 instance by id.\"\"\" ec2 = boto3 . resource ( \"ec2\" , region_name = region_name ) try : ec2 . Instance ( instance_id ). terminate () except ClientError as err : LOG . warning ( \"%s\" , str ( err )) if err . response [ \"Error\" ][ \"Code\" ] == \"OperationNotPermitted\" : return \"<!here|here> Warning! Modify termination protection: \" + instance_id else : return \"Uncaught error occured on termination: \" + instance_id return \"Successfully terminated\"","title":"terminate_instance"},{"location":"reference/helpers/aws_general/#try_except_status","text":"def try_except_status ( bo3_client_method , fail_str ) Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way View Source def try_except_status ( bo3_client_method , fail_str ): \"\"\"Takes a partially applied fuction passed to it so that it catches status codes/errors in a generalised way \"\"\" try : get_status = bo3_client_method status = get_status ()[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] except ClientError as err : LOG . warning ( fail_str , str ( err )) if err . response [ \"Error\" ][ \"Code\" ]: status = err . response [ \"Error\" ][ \"Code\" ] else : status = str ( err ) return status","title":"try_except_status"},{"location":"reference/helpers/aws_general/#upload_logs_s3","text":"def upload_logs_s3 ( bucket_name , log_name ) Upload log data to s3 bucket. View Source def upload_logs_s3 ( bucket_name , log_name ): \"\"\"Upload log data to s3 bucket.\"\"\" data = open ( log_name , \"rb\" ) s3_client = boto3 . resource ( \"s3\" ) try : s3_client . Bucket ( bucket_name ). put_object ( Key = log_name , Body = data ) LOG . info ( \"Uploading log file %s to s3 %s\" , log_name , bucket_name ) except Exception as err : LOG . warning ( \"Failed to Upload log file %s to s3 %s\" , log_name , bucket_name ) LOG . critical ( \"error: %s\" , err ) return True","title":"upload_logs_s3"},{"location":"reference/helpers/check_env_vars/","text":"Module helpers.check_env_vars View Source import os import sys import logging def check_env_vars (): \"\"\"Check aws & github env variables required to run this script.\"\"\" AWS_ACCESS_KEY_ID = os . environ . get ( \"AWS_ACCESS_KEY_ID\" , None ) AWS_SECRET_ACCESS_KEY = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None) # GITHUB_TOKEN = \"\" if ( AWS_ACCESS_KEY_ID is None ) or ( AWS_SECRET_ACCESS_KEY is None ): LOG . critical ( \"AWS access keys not set. Exiting..\" ) sys . exit ( 0 ) LOG . info ( \"AWS config good. Continuing...\" ) Functions check_env_vars def check_env_vars ( ) Check aws & github env variables required to run this script. View Source def check_env_vars (): \"\"\"Check aws & github env variables required to run this script.\"\"\" AWS_ACCESS_KEY_ID = os . environ . get ( \"AWS_ACCESS_KEY_ID\" , None ) AWS_SECRET_ACCESS_KEY = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = \"\" if ( AWS_ACCESS_KEY_ID is None ) or ( AWS_SECRET_ACCESS_KEY is None ): LOG . critical ( \"AWS access keys not set. Exiting..\" ) sys . exit ( 0 ) LOG . info ( \"AWS config good. Continuing...\" )","title":"Check Env Vars"},{"location":"reference/helpers/check_env_vars/#module-helperscheck_env_vars","text":"View Source import os import sys import logging def check_env_vars (): \"\"\"Check aws & github env variables required to run this script.\"\"\" AWS_ACCESS_KEY_ID = os . environ . get ( \"AWS_ACCESS_KEY_ID\" , None ) AWS_SECRET_ACCESS_KEY = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None) # GITHUB_TOKEN = \"\" if ( AWS_ACCESS_KEY_ID is None ) or ( AWS_SECRET_ACCESS_KEY is None ): LOG . critical ( \"AWS access keys not set. Exiting..\" ) sys . exit ( 0 ) LOG . info ( \"AWS config good. Continuing...\" )","title":"Module helpers.check_env_vars"},{"location":"reference/helpers/check_env_vars/#functions","text":"","title":"Functions"},{"location":"reference/helpers/check_env_vars/#check_env_vars","text":"def check_env_vars ( ) Check aws & github env variables required to run this script. View Source def check_env_vars (): \"\"\"Check aws & github env variables required to run this script.\"\"\" AWS_ACCESS_KEY_ID = os . environ . get ( \"AWS_ACCESS_KEY_ID\" , None ) AWS_SECRET_ACCESS_KEY = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = os . environ . get ( \"AWS_SECRET_ACCESS_KEY\" , None ) # GITHUB_TOKEN = \"\" if ( AWS_ACCESS_KEY_ID is None ) or ( AWS_SECRET_ACCESS_KEY is None ): LOG . critical ( \"AWS access keys not set. Exiting..\" ) sys . exit ( 0 ) LOG . info ( \"AWS config good. Continuing...\" )","title":"check_env_vars"},{"location":"reference/helpers/data_structure_search/","text":"Module helpers.data_structure_search View Source import json from pprint import pprint import sys import collections def find_nested_key ( my_json , find_key ): \"\"\"Search in a nested dict for a key or None.\"\"\" if find_key in my_json : return my_json [ find_key ] for _ , vval in my_json . items (): if isinstance ( vval , dict ): item = find_nested_key ( vval , find_key ) if item is not None : return item return None def nested_dic_update ( base_dic , in_dic ): \"\"\" Recursively merge two nested dictionaries together.\"\"\" for kkey , vval in in_dic . items (): if isinstance ( vval , collections . Mapping ): recursive_call = nested_dic_update ( base_dic . get ( kkey , {}), vval ) base_dic [ kkey ] = recursive_call else : base_dic [ kkey ] = in_dic [ kkey ] return base_dic def find_replace_dict_values ( mydict , search_key , old_value , new_value ): \"Recursively find and replace values in a nested dict\" for key , val in mydict . items (): if key == search_key : if val == old_value : mydict [ key ] = new_value elif isinstance ( mydict [ key ], dict ): find_replace_dict_values ( mydict [ key ], search_key , old_value , new_value ) return mydict Functions find_nested_key def find_nested_key ( my_json , find_key ) Search in a nested dict for a key or None. View Source def find_nested_key ( my_json , find_key ) : \"\"\"Search in a nested dict for a key or None.\"\"\" if find_key in my_json : return my_json [ find_key ] for _ , vval in my_json . items () : if isinstance ( vval , dict ) : item = find_nested_key ( vval , find_key ) if item is not None : return item return None find_replace_dict_values def find_replace_dict_values ( mydict , search_key , old_value , new_value ) Recursively find and replace values in a nested dict View Source def find_replace_dict_values ( mydict , search_key , old_value , new_value ) : \"Recursively find and replace values in a nested dict\" for key , val in mydict . items () : if key == search_key : if val == old_value : mydict [ key ] = new_value elif isinstance ( mydict [ key ] , dict ) : find_replace_dict_values ( mydict [ key ] , search_key , old_value , new_value ) return mydict nested_dic_update def nested_dic_update ( base_dic , in_dic ) Recursively merge two nested dictionaries together. View Source def nested_dic_update ( base_dic , in_dic ) : \"\"\" Recursively merge two nested dictionaries together.\"\"\" for kkey , vval in in_dic . items () : if isinstance ( vval , collections . Mapping ) : recursive_call = nested_dic_update ( base_dic . get ( kkey , {} ), vval ) base_dic [ kkey ] = recursive_call else : base_dic [ kkey ] = in_dic [ kkey ] return base_dic","title":"Data Structure Search"},{"location":"reference/helpers/data_structure_search/#module-helpersdata_structure_search","text":"View Source import json from pprint import pprint import sys import collections def find_nested_key ( my_json , find_key ): \"\"\"Search in a nested dict for a key or None.\"\"\" if find_key in my_json : return my_json [ find_key ] for _ , vval in my_json . items (): if isinstance ( vval , dict ): item = find_nested_key ( vval , find_key ) if item is not None : return item return None def nested_dic_update ( base_dic , in_dic ): \"\"\" Recursively merge two nested dictionaries together.\"\"\" for kkey , vval in in_dic . items (): if isinstance ( vval , collections . Mapping ): recursive_call = nested_dic_update ( base_dic . get ( kkey , {}), vval ) base_dic [ kkey ] = recursive_call else : base_dic [ kkey ] = in_dic [ kkey ] return base_dic def find_replace_dict_values ( mydict , search_key , old_value , new_value ): \"Recursively find and replace values in a nested dict\" for key , val in mydict . items (): if key == search_key : if val == old_value : mydict [ key ] = new_value elif isinstance ( mydict [ key ], dict ): find_replace_dict_values ( mydict [ key ], search_key , old_value , new_value ) return mydict","title":"Module helpers.data_structure_search"},{"location":"reference/helpers/data_structure_search/#functions","text":"","title":"Functions"},{"location":"reference/helpers/data_structure_search/#find_nested_key","text":"def find_nested_key ( my_json , find_key ) Search in a nested dict for a key or None. View Source def find_nested_key ( my_json , find_key ) : \"\"\"Search in a nested dict for a key or None.\"\"\" if find_key in my_json : return my_json [ find_key ] for _ , vval in my_json . items () : if isinstance ( vval , dict ) : item = find_nested_key ( vval , find_key ) if item is not None : return item return None","title":"find_nested_key"},{"location":"reference/helpers/data_structure_search/#find_replace_dict_values","text":"def find_replace_dict_values ( mydict , search_key , old_value , new_value ) Recursively find and replace values in a nested dict View Source def find_replace_dict_values ( mydict , search_key , old_value , new_value ) : \"Recursively find and replace values in a nested dict\" for key , val in mydict . items () : if key == search_key : if val == old_value : mydict [ key ] = new_value elif isinstance ( mydict [ key ] , dict ) : find_replace_dict_values ( mydict [ key ] , search_key , old_value , new_value ) return mydict","title":"find_replace_dict_values"},{"location":"reference/helpers/data_structure_search/#nested_dic_update","text":"def nested_dic_update ( base_dic , in_dic ) Recursively merge two nested dictionaries together. View Source def nested_dic_update ( base_dic , in_dic ) : \"\"\" Recursively merge two nested dictionaries together.\"\"\" for kkey , vval in in_dic . items () : if isinstance ( vval , collections . Mapping ) : recursive_call = nested_dic_update ( base_dic . get ( kkey , {} ), vval ) base_dic [ kkey ] = recursive_call else : base_dic [ kkey ] = in_dic [ kkey ] return base_dic","title":"nested_dic_update"},{"location":"reference/helpers/date_tools/","text":"Module helpers.date_tools Helpers to manipulate times View Source \"\"\"Helpers to manipulate times\"\"\" import os from datetime import datetime import arrow import sys import os import logging from datetime import datetime , timedelta import re from dateutil import tz from pydash import get LOG = logging . getLogger ( __name__ ) def get_unix_ts_unit ( unix_ts ): \"\"\"Check if linux timestamp units are in seconds or ms.\"\"\" if len ( unix_ts ) == 13 : unit = \"ms\" elif len ( unix_ts ) == 10 : unit = \"s\" return unit def convert_unix_time ( unix_time ): \"\"\"Convert unix timestamp to date-time object.\"\"\" return datetime . fromtimestamp ( unix_time / 1000 , tz = tz . tzutc ()) def format_dt_object ( dt_obj , fmt_str ): \"\"\"return date time object in format specified.\"\"\" return datetime . strftime ( dt_obj , fmt_str ) def check_date_fmt ( date_string = None ): \"\"\"Check that date matches format \"YYYY-MM-DD\".\"\"\" if re . match ( REGEX_PATTERN , date_string ): LOG . debug ( \"date string verified\" ) return True LOG . debug ( \"Whoops! Incorrect date format\" ) return False def string_to_datetime ( date_str , time_format ): \"\"\"Convert pre-checked string into datetime object.\"\"\" datet_obj = datetime . strptime ( date_str , time_format ) return datet_obj def calc_ndays_fwd ( ndays ): \"\"\"Returns a date string n days from today for use in an AWS schedule_deletion tag.\"\"\" utc_time_now = datetime . utcnow () ndays_fwd = utc_time_now . date () + timedelta ( days = ndays ) ndays_fwd_str = format_dt_object ( ndays_fwd , \"%Y-%m- %d \" ) return ndays_fwd_str def calc_ndays_back_from_today ( days_back ): \"\"\"Calculates the difference bewteen today and ndays back.\"\"\" utc_time_now = datetime . utcnow () . replace ( tzinfo = tz . tzutc ()) ndays_back = utc_time_now - timedelta ( days = days_back ) return ndays_back def calc_days_from_2dates_diff ( prev_date ): \"\"\"Calculates the difference bewteen today and a previous date in days.\"\"\" utc_time_now = datetime . utcnow () . replace ( tzinfo = tz . tzutc ()) # ndays_back = utc_time_now - timedelta(days=prev_date) ndays_back = utc_time_now - prev_date return ndays_back . days def datetimeformat ( date_str ): \"\"\"Print relative time from date_str.\"\"\" mydate_time = arrow . get ( date_str ) return mydate_time . humanize () def unixtime_to_datestring ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . fromtimestamp ( unix_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) def unixtime_to_datetime ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . utcfromtimestamp ( unix_time ) # TODO: unify redundant time functions def time_from_string ( mytime_str , tformat = \"%H:%M\" ): \"\"\"Converts a string into a time object.\"\"\" return datetime . strptime ( mytime_str , tformat ) . time () def date_from_string ( mydate_str ): \"\"\"Converts a string into a date object.\"\"\" return datetime . strptime ( mydate_str , \"%Y-%m- %d \" ) . date () # TODO: Move to another module def find_nested_key ( aws_response , key_path ): \"\"\"Check if nested mixed dictionary/list object has dict key. Uses pydash get\"\"\" return get ( aws_response , key_path ) Variables LOG Functions calc_days_from_2dates_diff def calc_days_from_2dates_diff ( prev_date ) Calculates the difference bewteen today and a previous date in days. View Source def calc_days_from_2dates_diff ( prev_date ): \"\"\"Calculates the difference bewteen today and a previous date in days.\"\"\" utc_time_now = datetime . utcnow (). replace ( tzinfo = tz . tzutc ()) # ndays_back = utc_time_now - timedelta ( days = prev_date ) ndays_back = utc_time_now - prev_date return ndays_back . days calc_ndays_back_from_today def calc_ndays_back_from_today ( days_back ) Calculates the difference bewteen today and ndays back. View Source def calc_ndays_back_from_today ( days_back ): \"\"\"Calculates the difference bewteen today and ndays back.\"\"\" utc_time_now = datetime . utcnow (). replace ( tzinfo = tz . tzutc ()) ndays_back = utc_time_now - timedelta ( days = days_back ) return ndays_back calc_ndays_fwd def calc_ndays_fwd ( ndays ) Returns a date string n days from today for use in an AWS schedule_deletion tag. View Source def calc_ndays_fwd ( ndays ): \"\"\"Returns a date string n days from today for use in an AWS schedule_deletion tag.\"\"\" utc_time_now = datetime . utcnow () ndays_fwd = utc_time_now . date () + timedelta ( days = ndays ) ndays_fwd_str = format_dt_object ( ndays_fwd , \"%Y-%m-%d\" ) return ndays_fwd_str check_date_fmt def check_date_fmt ( date_string = None ) Check that date matches format \"YYYY-MM-DD\". View Source def check_date_fmt ( date_string = None ): \"\"\"Check that date matches format \" YYYY - MM - DD \".\"\"\" if re . match ( REGEX_PATTERN , date_string ): LOG . debug ( \"date string verified\" ) return True LOG . debug ( \"Whoops! Incorrect date format\" ) return False convert_unix_time def convert_unix_time ( unix_time ) Convert unix timestamp to date-time object. View Source def convert_unix_time ( unix_time ): \"\"\"Convert unix timestamp to date-time object.\"\"\" return datetime . fromtimestamp ( unix_time / 1000 , tz = tz . tzutc ()) date_from_string def date_from_string ( mydate_str ) Converts a string into a date object. View Source def date_from_string ( mydate_str ): \"\"\"Converts a string into a date object.\"\"\" return datetime . strptime ( mydate_str , \"%Y-%m-%d\" ). date () datetimeformat def datetimeformat ( date_str ) Print relative time from date_str. View Source def datetimeformat ( date_str ): \"\"\"Print relative time from date_str.\"\"\" mydate_time = arrow . get ( date_str ) return mydate_time . humanize () find_nested_key def find_nested_key ( aws_response , key_path ) Check if nested mixed dictionary/list object has dict key. Uses pydash get View Source def find_nested_key ( aws_response , key_path ): \"\"\"Check if nested mixed dictionary/list object has dict key. Uses pydash get\"\"\" return get ( aws_response , key_path ) format_dt_object def format_dt_object ( dt_obj , fmt_str ) return date time object in format specified. View Source def format_dt_object ( dt_obj , fmt_str ): \"\"\"return date time object in format specified.\"\"\" return datetime . strftime ( dt_obj , fmt_str ) get_unix_ts_unit def get_unix_ts_unit ( unix_ts ) Check if linux timestamp units are in seconds or ms. View Source def get_unix_ts_unit ( unix_ts ): \"\"\"Check if linux timestamp units are in seconds or ms.\"\"\" if len ( unix_ts ) == 13 : unit = \"ms\" elif len ( unix_ts ) == 10 : unit = \"s\" return unit string_to_datetime def string_to_datetime ( date_str , time_format ) Convert pre-checked string into datetime object. View Source def string_to_datetime ( date_str , time_format ): \"\"\"Convert pre-checked string into datetime object.\"\"\" datet_obj = datetime . strptime ( date_str , time_format ) return datet_obj time_from_string def time_from_string ( mytime_str , tformat = '%H:%M' ) Converts a string into a time object. View Source def time_from_string ( mytime_str , tformat = \"%H:%M\" ): \"\"\"Converts a string into a time object.\"\"\" return datetime . strptime ( mytime_str , tformat ). time () unixtime_to_datestring def unixtime_to_datestring ( unix_time ) Convert unix timestamp into a string. View Source def unixtime_to_datestring ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . fromtimestamp ( unix_time ). strftime ( \"%Y-%m-%d %H:%M:%S\" ) unixtime_to_datetime def unixtime_to_datetime ( unix_time ) Convert unix timestamp into a string. View Source def unixtime_to_datetime ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . utcfromtimestamp ( unix_time )","title":"Date Tools"},{"location":"reference/helpers/date_tools/#module-helpersdate_tools","text":"Helpers to manipulate times View Source \"\"\"Helpers to manipulate times\"\"\" import os from datetime import datetime import arrow import sys import os import logging from datetime import datetime , timedelta import re from dateutil import tz from pydash import get LOG = logging . getLogger ( __name__ ) def get_unix_ts_unit ( unix_ts ): \"\"\"Check if linux timestamp units are in seconds or ms.\"\"\" if len ( unix_ts ) == 13 : unit = \"ms\" elif len ( unix_ts ) == 10 : unit = \"s\" return unit def convert_unix_time ( unix_time ): \"\"\"Convert unix timestamp to date-time object.\"\"\" return datetime . fromtimestamp ( unix_time / 1000 , tz = tz . tzutc ()) def format_dt_object ( dt_obj , fmt_str ): \"\"\"return date time object in format specified.\"\"\" return datetime . strftime ( dt_obj , fmt_str ) def check_date_fmt ( date_string = None ): \"\"\"Check that date matches format \"YYYY-MM-DD\".\"\"\" if re . match ( REGEX_PATTERN , date_string ): LOG . debug ( \"date string verified\" ) return True LOG . debug ( \"Whoops! Incorrect date format\" ) return False def string_to_datetime ( date_str , time_format ): \"\"\"Convert pre-checked string into datetime object.\"\"\" datet_obj = datetime . strptime ( date_str , time_format ) return datet_obj def calc_ndays_fwd ( ndays ): \"\"\"Returns a date string n days from today for use in an AWS schedule_deletion tag.\"\"\" utc_time_now = datetime . utcnow () ndays_fwd = utc_time_now . date () + timedelta ( days = ndays ) ndays_fwd_str = format_dt_object ( ndays_fwd , \"%Y-%m- %d \" ) return ndays_fwd_str def calc_ndays_back_from_today ( days_back ): \"\"\"Calculates the difference bewteen today and ndays back.\"\"\" utc_time_now = datetime . utcnow () . replace ( tzinfo = tz . tzutc ()) ndays_back = utc_time_now - timedelta ( days = days_back ) return ndays_back def calc_days_from_2dates_diff ( prev_date ): \"\"\"Calculates the difference bewteen today and a previous date in days.\"\"\" utc_time_now = datetime . utcnow () . replace ( tzinfo = tz . tzutc ()) # ndays_back = utc_time_now - timedelta(days=prev_date) ndays_back = utc_time_now - prev_date return ndays_back . days def datetimeformat ( date_str ): \"\"\"Print relative time from date_str.\"\"\" mydate_time = arrow . get ( date_str ) return mydate_time . humanize () def unixtime_to_datestring ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . fromtimestamp ( unix_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) def unixtime_to_datetime ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . utcfromtimestamp ( unix_time ) # TODO: unify redundant time functions def time_from_string ( mytime_str , tformat = \"%H:%M\" ): \"\"\"Converts a string into a time object.\"\"\" return datetime . strptime ( mytime_str , tformat ) . time () def date_from_string ( mydate_str ): \"\"\"Converts a string into a date object.\"\"\" return datetime . strptime ( mydate_str , \"%Y-%m- %d \" ) . date () # TODO: Move to another module def find_nested_key ( aws_response , key_path ): \"\"\"Check if nested mixed dictionary/list object has dict key. Uses pydash get\"\"\" return get ( aws_response , key_path )","title":"Module helpers.date_tools"},{"location":"reference/helpers/date_tools/#variables","text":"LOG","title":"Variables"},{"location":"reference/helpers/date_tools/#functions","text":"","title":"Functions"},{"location":"reference/helpers/date_tools/#calc_days_from_2dates_diff","text":"def calc_days_from_2dates_diff ( prev_date ) Calculates the difference bewteen today and a previous date in days. View Source def calc_days_from_2dates_diff ( prev_date ): \"\"\"Calculates the difference bewteen today and a previous date in days.\"\"\" utc_time_now = datetime . utcnow (). replace ( tzinfo = tz . tzutc ()) # ndays_back = utc_time_now - timedelta ( days = prev_date ) ndays_back = utc_time_now - prev_date return ndays_back . days","title":"calc_days_from_2dates_diff"},{"location":"reference/helpers/date_tools/#calc_ndays_back_from_today","text":"def calc_ndays_back_from_today ( days_back ) Calculates the difference bewteen today and ndays back. View Source def calc_ndays_back_from_today ( days_back ): \"\"\"Calculates the difference bewteen today and ndays back.\"\"\" utc_time_now = datetime . utcnow (). replace ( tzinfo = tz . tzutc ()) ndays_back = utc_time_now - timedelta ( days = days_back ) return ndays_back","title":"calc_ndays_back_from_today"},{"location":"reference/helpers/date_tools/#calc_ndays_fwd","text":"def calc_ndays_fwd ( ndays ) Returns a date string n days from today for use in an AWS schedule_deletion tag. View Source def calc_ndays_fwd ( ndays ): \"\"\"Returns a date string n days from today for use in an AWS schedule_deletion tag.\"\"\" utc_time_now = datetime . utcnow () ndays_fwd = utc_time_now . date () + timedelta ( days = ndays ) ndays_fwd_str = format_dt_object ( ndays_fwd , \"%Y-%m-%d\" ) return ndays_fwd_str","title":"calc_ndays_fwd"},{"location":"reference/helpers/date_tools/#check_date_fmt","text":"def check_date_fmt ( date_string = None ) Check that date matches format \"YYYY-MM-DD\". View Source def check_date_fmt ( date_string = None ): \"\"\"Check that date matches format \" YYYY - MM - DD \".\"\"\" if re . match ( REGEX_PATTERN , date_string ): LOG . debug ( \"date string verified\" ) return True LOG . debug ( \"Whoops! Incorrect date format\" ) return False","title":"check_date_fmt"},{"location":"reference/helpers/date_tools/#convert_unix_time","text":"def convert_unix_time ( unix_time ) Convert unix timestamp to date-time object. View Source def convert_unix_time ( unix_time ): \"\"\"Convert unix timestamp to date-time object.\"\"\" return datetime . fromtimestamp ( unix_time / 1000 , tz = tz . tzutc ())","title":"convert_unix_time"},{"location":"reference/helpers/date_tools/#date_from_string","text":"def date_from_string ( mydate_str ) Converts a string into a date object. View Source def date_from_string ( mydate_str ): \"\"\"Converts a string into a date object.\"\"\" return datetime . strptime ( mydate_str , \"%Y-%m-%d\" ). date ()","title":"date_from_string"},{"location":"reference/helpers/date_tools/#datetimeformat","text":"def datetimeformat ( date_str ) Print relative time from date_str. View Source def datetimeformat ( date_str ): \"\"\"Print relative time from date_str.\"\"\" mydate_time = arrow . get ( date_str ) return mydate_time . humanize ()","title":"datetimeformat"},{"location":"reference/helpers/date_tools/#find_nested_key","text":"def find_nested_key ( aws_response , key_path ) Check if nested mixed dictionary/list object has dict key. Uses pydash get View Source def find_nested_key ( aws_response , key_path ): \"\"\"Check if nested mixed dictionary/list object has dict key. Uses pydash get\"\"\" return get ( aws_response , key_path )","title":"find_nested_key"},{"location":"reference/helpers/date_tools/#format_dt_object","text":"def format_dt_object ( dt_obj , fmt_str ) return date time object in format specified. View Source def format_dt_object ( dt_obj , fmt_str ): \"\"\"return date time object in format specified.\"\"\" return datetime . strftime ( dt_obj , fmt_str )","title":"format_dt_object"},{"location":"reference/helpers/date_tools/#get_unix_ts_unit","text":"def get_unix_ts_unit ( unix_ts ) Check if linux timestamp units are in seconds or ms. View Source def get_unix_ts_unit ( unix_ts ): \"\"\"Check if linux timestamp units are in seconds or ms.\"\"\" if len ( unix_ts ) == 13 : unit = \"ms\" elif len ( unix_ts ) == 10 : unit = \"s\" return unit","title":"get_unix_ts_unit"},{"location":"reference/helpers/date_tools/#string_to_datetime","text":"def string_to_datetime ( date_str , time_format ) Convert pre-checked string into datetime object. View Source def string_to_datetime ( date_str , time_format ): \"\"\"Convert pre-checked string into datetime object.\"\"\" datet_obj = datetime . strptime ( date_str , time_format ) return datet_obj","title":"string_to_datetime"},{"location":"reference/helpers/date_tools/#time_from_string","text":"def time_from_string ( mytime_str , tformat = '%H:%M' ) Converts a string into a time object. View Source def time_from_string ( mytime_str , tformat = \"%H:%M\" ): \"\"\"Converts a string into a time object.\"\"\" return datetime . strptime ( mytime_str , tformat ). time ()","title":"time_from_string"},{"location":"reference/helpers/date_tools/#unixtime_to_datestring","text":"def unixtime_to_datestring ( unix_time ) Convert unix timestamp into a string. View Source def unixtime_to_datestring ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . fromtimestamp ( unix_time ). strftime ( \"%Y-%m-%d %H:%M:%S\" )","title":"unixtime_to_datestring"},{"location":"reference/helpers/date_tools/#unixtime_to_datetime","text":"def unixtime_to_datetime ( unix_time ) Convert unix timestamp into a string. View Source def unixtime_to_datetime ( unix_time ): \"\"\"Convert unix timestamp into a string.\"\"\" return datetime . utcfromtimestamp ( unix_time )","title":"unixtime_to_datetime"},{"location":"reference/helpers/dateframe_tools/","text":"Module helpers.dateframe_tools Helpers to manipulate dataframes View Source \"\"\"Helpers to manipulate dataframes\"\"\" import os from datetime import datetime import pandas as pd import logging from helpers.date_tools import unixtime_to_datetime import os from datetime import datetime import pandas as pd LOG = logging . getLogger ( __name__ ) def rm_col_names ( _df , exclude_list ): \"\"\"Take dataframe column names and remove those in supplied exclude use.\"\"\" col_names = list ( _df . columns . values ) for excl_str in exclude_list : try : col_names . remove ( excl_str ) except KeyError : pass return col_names def drop_cols ( _df , drop_list ): \"\"\"Drop columns from dataframe supplied in drop_list.\"\"\" try : _df . drop ( drop_list , axis = 1 , inplace = True ) except ValueError : pass return _df def read_data_sim ( infile ): \"\"\"Read simulation csv file.\"\"\" sim_ts_data = pd . read_csv ( infile , sep = \",\" , header = 0 ) return sim_ts_data def slice_dataframe ( _df , start , finish ): \"\"\"Prob deprecated.\"\"\" sliced = _df [ start : finish ] . copy () slice_timestamp = unixtime_to_datetime ( sliced [ \"binned_ts\" ] . iloc [ 0 ]) return ( sliced , slice_timestamp ) def slice_dataframe2 ( _df , start , finish ): \"\"\"Slice simulation dataframe into windowed segments.\"\"\" sliced = _df [ start : finish ] . copy () return sliced def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO: this is repeated in rm_col_names (generalise) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return def set_time_as_index ( _df ): \"\"\"Set time column as dataframe index and then drop said column.\"\"\" _df [ \"time\" ] = pd . to_datetime ( _df [ \"time\" ], format = \"%H:%M:%S\" ) . dt . time _df . index = _df [ \"time\" ] _df . drop ([ \"time\" ], axis = 1 , inplace = True ) return _df Variables LOG Functions delfiles_not_in_list def delfiles_not_in_list ( folder , exclude_list ) Delete all files in folder aprt from those in exclude list. View Source def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO : this is repeated in rm_col_names ( generalise ) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return drop_cols def drop_cols ( _df , drop_list ) Drop columns from dataframe supplied in drop_list. View Source def drop_cols ( _df , drop_list ): \"\"\"Drop columns from dataframe supplied in drop_list.\"\"\" try : _df . drop ( drop_list , axis = 1 , inplace = True ) except ValueError : pass return _df read_data_sim def read_data_sim ( infile ) Read simulation csv file. View Source def read_data_sim ( infile ): \"\"\"Read simulation csv file.\"\"\" sim_ts_data = pd . read_csv ( infile , sep = \",\" , header = 0 ) return sim_ts_data rm_col_names def rm_col_names ( _df , exclude_list ) Take dataframe column names and remove those in supplied exclude use. View Source def rm_col_names ( _df , exclude_list ): \"\"\"Take dataframe column names and remove those in supplied exclude use.\"\"\" col_names = list ( _df . columns . values ) for excl_str in exclude_list : try : col_names . remove ( excl_str ) except KeyError : pass return col_names set_time_as_index def set_time_as_index ( _df ) Set time column as dataframe index and then drop said column. View Source def set_time_as_index ( _df ): \"\"\"Set time column as dataframe index and then drop said column.\"\"\" _df [ \"time\" ] = pd . to_datetime ( _df [ \"time\" ], format = \"%H:%M:%S\" ). dt . time _df . index = _df [ \"time\" ] _df . drop ([ \"time\" ], axis = 1 , inplace = True ) return _df slice_dataframe def slice_dataframe ( _df , start , finish ) Prob deprecated. View Source def slice_dataframe ( _df , start , finish ): \"\"\"Prob deprecated.\"\"\" sliced = _df [ start : finish ]. copy () slice_timestamp = unixtime_to_datetime ( sliced [ \"binned_ts\" ]. iloc [ 0 ]) return ( sliced , slice_timestamp ) slice_dataframe2 def slice_dataframe2 ( _df , start , finish ) Slice simulation dataframe into windowed segments. View Source def slice_dataframe2 ( _df , start , finish ): \"\"\"Slice simulation dataframe into windowed segments.\"\"\" sliced = _df [ start : finish ]. copy () return sliced","title":"Dateframe Tools"},{"location":"reference/helpers/dateframe_tools/#module-helpersdateframe_tools","text":"Helpers to manipulate dataframes View Source \"\"\"Helpers to manipulate dataframes\"\"\" import os from datetime import datetime import pandas as pd import logging from helpers.date_tools import unixtime_to_datetime import os from datetime import datetime import pandas as pd LOG = logging . getLogger ( __name__ ) def rm_col_names ( _df , exclude_list ): \"\"\"Take dataframe column names and remove those in supplied exclude use.\"\"\" col_names = list ( _df . columns . values ) for excl_str in exclude_list : try : col_names . remove ( excl_str ) except KeyError : pass return col_names def drop_cols ( _df , drop_list ): \"\"\"Drop columns from dataframe supplied in drop_list.\"\"\" try : _df . drop ( drop_list , axis = 1 , inplace = True ) except ValueError : pass return _df def read_data_sim ( infile ): \"\"\"Read simulation csv file.\"\"\" sim_ts_data = pd . read_csv ( infile , sep = \",\" , header = 0 ) return sim_ts_data def slice_dataframe ( _df , start , finish ): \"\"\"Prob deprecated.\"\"\" sliced = _df [ start : finish ] . copy () slice_timestamp = unixtime_to_datetime ( sliced [ \"binned_ts\" ] . iloc [ 0 ]) return ( sliced , slice_timestamp ) def slice_dataframe2 ( _df , start , finish ): \"\"\"Slice simulation dataframe into windowed segments.\"\"\" sliced = _df [ start : finish ] . copy () return sliced def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO: this is repeated in rm_col_names (generalise) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return def set_time_as_index ( _df ): \"\"\"Set time column as dataframe index and then drop said column.\"\"\" _df [ \"time\" ] = pd . to_datetime ( _df [ \"time\" ], format = \"%H:%M:%S\" ) . dt . time _df . index = _df [ \"time\" ] _df . drop ([ \"time\" ], axis = 1 , inplace = True ) return _df","title":"Module helpers.dateframe_tools"},{"location":"reference/helpers/dateframe_tools/#variables","text":"LOG","title":"Variables"},{"location":"reference/helpers/dateframe_tools/#functions","text":"","title":"Functions"},{"location":"reference/helpers/dateframe_tools/#delfiles_not_in_list","text":"def delfiles_not_in_list ( folder , exclude_list ) Delete all files in folder aprt from those in exclude list. View Source def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO : this is repeated in rm_col_names ( generalise ) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return","title":"delfiles_not_in_list"},{"location":"reference/helpers/dateframe_tools/#drop_cols","text":"def drop_cols ( _df , drop_list ) Drop columns from dataframe supplied in drop_list. View Source def drop_cols ( _df , drop_list ): \"\"\"Drop columns from dataframe supplied in drop_list.\"\"\" try : _df . drop ( drop_list , axis = 1 , inplace = True ) except ValueError : pass return _df","title":"drop_cols"},{"location":"reference/helpers/dateframe_tools/#read_data_sim","text":"def read_data_sim ( infile ) Read simulation csv file. View Source def read_data_sim ( infile ): \"\"\"Read simulation csv file.\"\"\" sim_ts_data = pd . read_csv ( infile , sep = \",\" , header = 0 ) return sim_ts_data","title":"read_data_sim"},{"location":"reference/helpers/dateframe_tools/#rm_col_names","text":"def rm_col_names ( _df , exclude_list ) Take dataframe column names and remove those in supplied exclude use. View Source def rm_col_names ( _df , exclude_list ): \"\"\"Take dataframe column names and remove those in supplied exclude use.\"\"\" col_names = list ( _df . columns . values ) for excl_str in exclude_list : try : col_names . remove ( excl_str ) except KeyError : pass return col_names","title":"rm_col_names"},{"location":"reference/helpers/dateframe_tools/#set_time_as_index","text":"def set_time_as_index ( _df ) Set time column as dataframe index and then drop said column. View Source def set_time_as_index ( _df ): \"\"\"Set time column as dataframe index and then drop said column.\"\"\" _df [ \"time\" ] = pd . to_datetime ( _df [ \"time\" ], format = \"%H:%M:%S\" ). dt . time _df . index = _df [ \"time\" ] _df . drop ([ \"time\" ], axis = 1 , inplace = True ) return _df","title":"set_time_as_index"},{"location":"reference/helpers/dateframe_tools/#slice_dataframe","text":"def slice_dataframe ( _df , start , finish ) Prob deprecated. View Source def slice_dataframe ( _df , start , finish ): \"\"\"Prob deprecated.\"\"\" sliced = _df [ start : finish ]. copy () slice_timestamp = unixtime_to_datetime ( sliced [ \"binned_ts\" ]. iloc [ 0 ]) return ( sliced , slice_timestamp )","title":"slice_dataframe"},{"location":"reference/helpers/dateframe_tools/#slice_dataframe2","text":"def slice_dataframe2 ( _df , start , finish ) Slice simulation dataframe into windowed segments. View Source def slice_dataframe2 ( _df , start , finish ): \"\"\"Slice simulation dataframe into windowed segments.\"\"\" sliced = _df [ start : finish ]. copy () return sliced","title":"slice_dataframe2"},{"location":"reference/helpers/dynamic_loader/","text":"Module helpers.dynamic_loader View Source import os import sys import logging import inspect import importlib LOG = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO , stream = sys . stdout ) def _get_def_files ( dirname ): files = [ fil . replace ( \".py\" , \"\" ) for fil in os . listdir ( dirname ) if fil . startswith ( \"test_\" ) ] LOG . info ( \"def files found: %s \" , files ) return files def _import_func_defs ( graphdef_files ): _modules = __import__ ( \"func_defs\" , globals (), locals (), graphdef_files , 0 ) _module_list = [ ( key , value ) for key , value in inspect . getmembers ( _modules ) if inspect . ismodule ( value ) ] LOG . info ( \"_module_list %s \" , _module_list ) func_dict = {} for module_name , _ in _module_list : # print(\"*\", module_name, val) mymodule = importlib . import_module ( \"func_defs.\" + module_name ) LOG . info ( \" module Loaded %s \" , mymodule . __name__ ) for oxo in inspect . getmembers ( mymodule ): # print(\"oxo\", oxo) if inspect . isfunction ( oxo [ 1 ]): # func_dict[\"function_name\"] = function func_name = oxo [ 1 ] . __name__ LOG . info ( \" func_name %s \" , func_name ) func_dict [ func_name ] = oxo [ 1 ] LOG . info ( \"No of function defs: %s \" , str ( len ( func_dict ))) return func_dict def load ( dirname ): _func_def_files = _get_def_files ( dirname ) return _import_func_defs ( _func_def_files ) Variables LOG Functions load def load ( dirname ) View Source def load ( dirname ): _func_def_files = _get_def_files ( dirname ) return _import_func_defs ( _func_def_files )","title":"Dynamic Loader"},{"location":"reference/helpers/dynamic_loader/#module-helpersdynamic_loader","text":"View Source import os import sys import logging import inspect import importlib LOG = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO , stream = sys . stdout ) def _get_def_files ( dirname ): files = [ fil . replace ( \".py\" , \"\" ) for fil in os . listdir ( dirname ) if fil . startswith ( \"test_\" ) ] LOG . info ( \"def files found: %s \" , files ) return files def _import_func_defs ( graphdef_files ): _modules = __import__ ( \"func_defs\" , globals (), locals (), graphdef_files , 0 ) _module_list = [ ( key , value ) for key , value in inspect . getmembers ( _modules ) if inspect . ismodule ( value ) ] LOG . info ( \"_module_list %s \" , _module_list ) func_dict = {} for module_name , _ in _module_list : # print(\"*\", module_name, val) mymodule = importlib . import_module ( \"func_defs.\" + module_name ) LOG . info ( \" module Loaded %s \" , mymodule . __name__ ) for oxo in inspect . getmembers ( mymodule ): # print(\"oxo\", oxo) if inspect . isfunction ( oxo [ 1 ]): # func_dict[\"function_name\"] = function func_name = oxo [ 1 ] . __name__ LOG . info ( \" func_name %s \" , func_name ) func_dict [ func_name ] = oxo [ 1 ] LOG . info ( \"No of function defs: %s \" , str ( len ( func_dict ))) return func_dict def load ( dirname ): _func_def_files = _get_def_files ( dirname ) return _import_func_defs ( _func_def_files )","title":"Module helpers.dynamic_loader"},{"location":"reference/helpers/dynamic_loader/#variables","text":"LOG","title":"Variables"},{"location":"reference/helpers/dynamic_loader/#functions","text":"","title":"Functions"},{"location":"reference/helpers/dynamic_loader/#load","text":"def load ( dirname ) View Source def load ( dirname ): _func_def_files = _get_def_files ( dirname ) return _import_func_defs ( _func_def_files )","title":"load"},{"location":"reference/helpers/file_tools/","text":"Module helpers.file_tools View Source import json def _find_my_yamls ( myc_folder , specific_files = None ): \"\"\"Load all .yaml files in a folder or a specific list of yaml files.\"\"\" selected_files = [] if specific_files : yaml_list = specific_files . replace ( \" \" , \"\" ) . split ( \",\" ) all_files = [ myc_folder + yaml for yaml in yaml_list ] else : all_files = glob . glob ( myc_folder + \"/*.yaml\" ) return all_files def string_list_writer ( file_name , list_holder , mode = None ): \"\"\"Writes an outfile froim a list of strings.\"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : for line_str in list_holder : file . write ( line_str + \" \\n \" ) def simple_text_writer ( file_name , text , mode = None ): \"\"\" Writes an outfile froim a list of strings. Mode = r, w, etc str. \"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : file . write ( text ) def load_local_json2 ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis.\"\"\" for line in data_file : json_thing = json . loads ( line ) return json_thing def load_local_json_generator ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis. Generator\"\"\" for line in data_file : json_thing = json . loads ( line ) yield json_thing Functions load_local_json2 def load_local_json2 ( data_file ) Return data from compact json file on a line-by-line basis. View Source def load_local_json2 ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis.\"\"\" for line in data_file : json_thing = json . loads ( line ) return json_thing load_local_json_generator def load_local_json_generator ( data_file ) Return data from compact json file on a line-by-line basis. Generator View Source def load_local_json_generator ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis. Generator\"\"\" for line in data_file : json_thing = json . loads ( line ) yield json_thing simple_text_writer def simple_text_writer ( file_name , text , mode = None ) Writes an outfile froim a list of strings. Mode = r, w, etc str. View Source def simple_text_writer ( file_name , text , mode = None ): \"\"\" Writes an outfile froim a list of strings. Mode = r, w, etc str. \"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : file . write ( text ) string_list_writer def string_list_writer ( file_name , list_holder , mode = None ) Writes an outfile froim a list of strings. View Source def string_list_writer ( file_name , list_holder , mode = None ): \"\"\"Writes an outfile froim a list of strings.\"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : for line_str in list_holder : file . write ( line_str + \"\\n\" )","title":"File Tools"},{"location":"reference/helpers/file_tools/#module-helpersfile_tools","text":"View Source import json def _find_my_yamls ( myc_folder , specific_files = None ): \"\"\"Load all .yaml files in a folder or a specific list of yaml files.\"\"\" selected_files = [] if specific_files : yaml_list = specific_files . replace ( \" \" , \"\" ) . split ( \",\" ) all_files = [ myc_folder + yaml for yaml in yaml_list ] else : all_files = glob . glob ( myc_folder + \"/*.yaml\" ) return all_files def string_list_writer ( file_name , list_holder , mode = None ): \"\"\"Writes an outfile froim a list of strings.\"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : for line_str in list_holder : file . write ( line_str + \" \\n \" ) def simple_text_writer ( file_name , text , mode = None ): \"\"\" Writes an outfile froim a list of strings. Mode = r, w, etc str. \"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : file . write ( text ) def load_local_json2 ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis.\"\"\" for line in data_file : json_thing = json . loads ( line ) return json_thing def load_local_json_generator ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis. Generator\"\"\" for line in data_file : json_thing = json . loads ( line ) yield json_thing","title":"Module helpers.file_tools"},{"location":"reference/helpers/file_tools/#functions","text":"","title":"Functions"},{"location":"reference/helpers/file_tools/#load_local_json2","text":"def load_local_json2 ( data_file ) Return data from compact json file on a line-by-line basis. View Source def load_local_json2 ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis.\"\"\" for line in data_file : json_thing = json . loads ( line ) return json_thing","title":"load_local_json2"},{"location":"reference/helpers/file_tools/#load_local_json_generator","text":"def load_local_json_generator ( data_file ) Return data from compact json file on a line-by-line basis. Generator View Source def load_local_json_generator ( data_file ): \"\"\"Return data from compact json file on a line-by-line basis. Generator\"\"\" for line in data_file : json_thing = json . loads ( line ) yield json_thing","title":"load_local_json_generator"},{"location":"reference/helpers/file_tools/#simple_text_writer","text":"def simple_text_writer ( file_name , text , mode = None ) Writes an outfile froim a list of strings. Mode = r, w, etc str. View Source def simple_text_writer ( file_name , text , mode = None ): \"\"\" Writes an outfile froim a list of strings. Mode = r, w, etc str. \"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : file . write ( text )","title":"simple_text_writer"},{"location":"reference/helpers/file_tools/#string_list_writer","text":"def string_list_writer ( file_name , list_holder , mode = None ) Writes an outfile froim a list of strings. View Source def string_list_writer ( file_name , list_holder , mode = None ): \"\"\"Writes an outfile froim a list of strings.\"\"\" if mode == None : mode = \"w\" with open ( file_name , mode ) as file : for line_str in list_holder : file . write ( line_str + \"\\n\" )","title":"string_list_writer"},{"location":"reference/helpers/os_functions/","text":"Module helpers.os_functions Python helpers module. View Source \"\"\"Python helpers module.\"\"\" import os import sys import logging import shutil LOG = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO , stream = sys . stdout ) def delete_file ( file_name ): \"\"\"Deletes a file from disk.\"\"\" try : os . remove ( file_name ) LOG . info ( \"Log file %s deleted from local drive\" , file_name ) except OSError as err : LOG . warning ( \"Log file %s does not exist %s \" , file_name , err ) def rm_dir_if_exists ( target ): \"\"\"Remove file directory if it exists.\"\"\" if os . path . exists ( target ): shutil . rmtree ( target ) def rm_fil_if_exists ( target ): # so we should check if file exists or not not before deleting them if os . path . exists ( target ): os . remove ( target ) else : LOG . warning ( \"Delete file attempt failed for %s \" , target ) def copy_clobber ( source , target ): \"\"\"Copy directory. Overwrite target folder if it exists.\"\"\" rm_dir_if_exists ( target ) shutil . copytree ( source , target ) LOG . info ( \"Copied: %s --> %s \" , source , target ) def select_os_things ( my_wd = \"./\" , mode = \"file\" , suf_pre = \"\" , exclude_list = []): \"\"\" Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. \"\"\" if mode == \"folder\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isdir ( thing ) if thing not in exclude_list ] elif mode == \"file\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isfile ( thing ) if thing not in exclude_list ] elif mode == \"suffix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . endswith ( suf_pre ) if thing not in exclude_list ] elif mode == \"prefix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . startswith ( suf_pre ) if thing not in exclude_list ] else : LOG . critical ( \"Incorrect mode: %s \" , mode ) sys . exit ( 0 ) return os_thing_list def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ) . replace ( \"-\" , \"\" ) . lower () return unifiedkey def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO: this is repeated in rm_col_names (generalise) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return Variables LOG Functions copy_clobber def copy_clobber ( source , target ) Copy directory. Overwrite target folder if it exists. View Source def copy_clobber ( source , target ): \"\"\"Copy directory. Overwrite target folder if it exists.\"\"\" rm_dir_if_exists ( target ) shutil . copytree ( source , target ) LOG . info ( \"Copied: %s --> %s\" , source , target ) count_str_whitespace def count_str_whitespace ( mystr ) Counts no of whitespace in string and returns an int. View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count delete_file def delete_file ( file_name ) Deletes a file from disk. View Source def delete_file ( file_name ): \"\"\"Deletes a file from disk.\"\"\" try : os . remove ( file_name ) LOG . info ( \"Log file %s deleted from local drive\" , file_name ) except OSError as err : LOG . warning ( \"Log file %s does not exist %s\" , file_name , err ) delfiles_not_in_list def delfiles_not_in_list ( folder , exclude_list ) Delete all files in folder aprt from those in exclude list. View Source def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO : this is repeated in rm_col_names ( generalise ) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return rm_dir_if_exists def rm_dir_if_exists ( target ) Remove file directory if it exists. View Source def rm_dir_if_exists ( target ): \"\"\"Remove file directory if it exists.\"\"\" if os . path . exists ( target ): shutil . rmtree ( target ) rm_fil_if_exists def rm_fil_if_exists ( target ) View Source def rm_fil_if_exists ( target ): # so we should check if file exists or not not before deleting them if os . path . exists ( target ): os . remove ( target ) else : LOG . warning ( \"Delete file attempt failed for %s\" , target ) select_os_things def select_os_things ( my_wd = './' , mode = 'file' , suf_pre = '' , exclude_list = [] ) Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. View Source def select_os_things ( my_wd = \"./\" , mode = \"file\" , suf_pre = \"\" , exclude_list = []): \"\"\" Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. \"\"\" if mode == \"folder\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isdir ( thing ) if thing not in exclude_list ] elif mode == \"file\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isfile ( thing ) if thing not in exclude_list ] elif mode == \"suffix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . endswith ( suf_pre ) if thing not in exclude_list ] elif mode == \"prefix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . startswith ( suf_pre ) if thing not in exclude_list ] else : LOG . critical ( \"Incorrect mode: %s\" , mode ) sys . exit ( 0 ) return os_thing_list unify_dic_key def unify_dic_key ( key ) View Source def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey","title":"Os Functions"},{"location":"reference/helpers/os_functions/#module-helpersos_functions","text":"Python helpers module. View Source \"\"\"Python helpers module.\"\"\" import os import sys import logging import shutil LOG = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO , stream = sys . stdout ) def delete_file ( file_name ): \"\"\"Deletes a file from disk.\"\"\" try : os . remove ( file_name ) LOG . info ( \"Log file %s deleted from local drive\" , file_name ) except OSError as err : LOG . warning ( \"Log file %s does not exist %s \" , file_name , err ) def rm_dir_if_exists ( target ): \"\"\"Remove file directory if it exists.\"\"\" if os . path . exists ( target ): shutil . rmtree ( target ) def rm_fil_if_exists ( target ): # so we should check if file exists or not not before deleting them if os . path . exists ( target ): os . remove ( target ) else : LOG . warning ( \"Delete file attempt failed for %s \" , target ) def copy_clobber ( source , target ): \"\"\"Copy directory. Overwrite target folder if it exists.\"\"\" rm_dir_if_exists ( target ) shutil . copytree ( source , target ) LOG . info ( \"Copied: %s --> %s \" , source , target ) def select_os_things ( my_wd = \"./\" , mode = \"file\" , suf_pre = \"\" , exclude_list = []): \"\"\" Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. \"\"\" if mode == \"folder\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isdir ( thing ) if thing not in exclude_list ] elif mode == \"file\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isfile ( thing ) if thing not in exclude_list ] elif mode == \"suffix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . endswith ( suf_pre ) if thing not in exclude_list ] elif mode == \"prefix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . startswith ( suf_pre ) if thing not in exclude_list ] else : LOG . critical ( \"Incorrect mode: %s \" , mode ) sys . exit ( 0 ) return os_thing_list def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ) . replace ( \"-\" , \"\" ) . lower () return unifiedkey def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO: this is repeated in rm_col_names (generalise) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return","title":"Module helpers.os_functions"},{"location":"reference/helpers/os_functions/#variables","text":"LOG","title":"Variables"},{"location":"reference/helpers/os_functions/#functions","text":"","title":"Functions"},{"location":"reference/helpers/os_functions/#copy_clobber","text":"def copy_clobber ( source , target ) Copy directory. Overwrite target folder if it exists. View Source def copy_clobber ( source , target ): \"\"\"Copy directory. Overwrite target folder if it exists.\"\"\" rm_dir_if_exists ( target ) shutil . copytree ( source , target ) LOG . info ( \"Copied: %s --> %s\" , source , target )","title":"copy_clobber"},{"location":"reference/helpers/os_functions/#count_str_whitespace","text":"def count_str_whitespace ( mystr ) Counts no of whitespace in string and returns an int. View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count","title":"count_str_whitespace"},{"location":"reference/helpers/os_functions/#delete_file","text":"def delete_file ( file_name ) Deletes a file from disk. View Source def delete_file ( file_name ): \"\"\"Deletes a file from disk.\"\"\" try : os . remove ( file_name ) LOG . info ( \"Log file %s deleted from local drive\" , file_name ) except OSError as err : LOG . warning ( \"Log file %s does not exist %s\" , file_name , err )","title":"delete_file"},{"location":"reference/helpers/os_functions/#delfiles_not_in_list","text":"def delfiles_not_in_list ( folder , exclude_list ) Delete all files in folder aprt from those in exclude list. View Source def delfiles_not_in_list ( folder , exclude_list ): \"\"\"Delete all files in folder aprt from those in exclude list.\"\"\" ### prepare delete list del_files = os . listdir ( folder ) # TODO : this is repeated in rm_col_names ( generalise ) for excl_str in exclude_list : try : del_files . remove ( excl_str ) except ( ValueError , KeyError ): pass ### delete files for rmfile in del_files : file_path = os . path . join ( folder , rmfile ) try : if os . path . isfile ( file_path ): os . unlink ( file_path ) except Exception as err : print ( err ) return","title":"delfiles_not_in_list"},{"location":"reference/helpers/os_functions/#rm_dir_if_exists","text":"def rm_dir_if_exists ( target ) Remove file directory if it exists. View Source def rm_dir_if_exists ( target ): \"\"\"Remove file directory if it exists.\"\"\" if os . path . exists ( target ): shutil . rmtree ( target )","title":"rm_dir_if_exists"},{"location":"reference/helpers/os_functions/#rm_fil_if_exists","text":"def rm_fil_if_exists ( target ) View Source def rm_fil_if_exists ( target ): # so we should check if file exists or not not before deleting them if os . path . exists ( target ): os . remove ( target ) else : LOG . warning ( \"Delete file attempt failed for %s\" , target )","title":"rm_fil_if_exists"},{"location":"reference/helpers/os_functions/#select_os_things","text":"def select_os_things ( my_wd = './' , mode = 'file' , suf_pre = '' , exclude_list = [] ) Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. View Source def select_os_things ( my_wd = \"./\" , mode = \"file\" , suf_pre = \"\" , exclude_list = []): \"\"\" Create a filtered list of os type items. Mode can be folder, file, suffix, prefix. \"\"\" if mode == \"folder\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isdir ( thing ) if thing not in exclude_list ] elif mode == \"file\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if os . path . isfile ( thing ) if thing not in exclude_list ] elif mode == \"suffix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . endswith ( suf_pre ) if thing not in exclude_list ] elif mode == \"prefix\" : os_thing_list = [ thing for thing in os . listdir ( my_wd ) if thing . startswith ( suf_pre ) if thing not in exclude_list ] else : LOG . critical ( \"Incorrect mode: %s\" , mode ) sys . exit ( 0 ) return os_thing_list","title":"select_os_things"},{"location":"reference/helpers/os_functions/#unify_dic_key","text":"def unify_dic_key ( key ) View Source def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey","title":"unify_dic_key"},{"location":"reference/helpers/plotter_tools/","text":"Module helpers.plotter_tools Functions to assist in plotting images. View Source \"\"\"Functions to assist in plotting images.\"\"\" import os as os import subprocess from collections import defaultdict from operator import itemgetter from itertools import zip_longest , groupby import matplotlib matplotlib . use ( \"agg\" ) import matplotlib.pyplot as plt import matplotlib.patheffects as pe from matplotlib import rcParams from pylab import figure rcParams [ \"axes.labelsize\" ] = 14 rcParams [ \"xtick.labelsize\" ] = 14 rcParams [ \"ytick.labelsize\" ] = 14 rcParams [ \"legend.fontsize\" ] = 12 rcParams [ \"font.family\" ] = \"Dejavu Sans\" rcParams [ \"font.serif\" ] = [ \"Computer Modern Roman\" ] rcParams [ \"xtick.major.pad\" ] = 12 rcParams [ \"ytick.major.pad\" ] = 12 def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ): \"\"\"Concats a list of images into an NxN page using imagemagick .\"\"\" horiz_paste = grouper ( img_list , horiz_n ) vert_img_li = [] for idx , row in enumerate ( horiz_paste ): img_name_li = [ path_str + \"/\" + img_name for img_name in row if img_name is not None ] out_img_name = path_str + \"/row_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"horiz\" ) vert_img_li . append ( out_img_name ) ### vert_paste = grouper ( vert_img_li , vert_n ) for idx , col in enumerate ( vert_paste ): img_name_li = [ row for row in col if row is not None ] out_img_name = path_str + fin_prefix + \"_page_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"vert\" ) return None def grouper ( mylist , subli_size ): \"\"\"Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\"\" args = [ iter ( mylist )] * subli_size return zip_longest ( * args , fillvalue = None ) def concat_images ( file_li , outname , mode = \"horiz\" , del_input = True ): \"\"\"Pastes inages into horizontal rows or vertical columns.\"\"\" if mode == \"horiz\" : append = \"+append\" elif mode == \"vert\" : append = \"-append\" formated_cmd = [ \"convert\" , append ] for file_name in file_li : formated_cmd . append ( file_name ) formated_cmd . append ( outname ) subprocess . call ( formated_cmd ) if del_input : for file_name in file_li : os . remove ( file_name ) return def nested_line_plot ( x_data = None , y_data = None , ls_var = \"--\" , col_var = \"RAND\" , lw_var = 1.0 , alpha1_var = 0.5 , title_var = \"null\" , fig_name = \"null.png\" , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ): \"\"\" Multi-line function call. kwargs are dictionaries. \"\"\" fig = figure () plt . xticks ( rotation = 15 ) plt . title ( title_var ) axhell = plt . subplot ( 1 , 1 , 1 ) axhell . grid ( True ) axhell . set_title ( title_var ) axhell . axhline ( y = 2.0 , color = \"grey\" , linestyle = \"dashed\" , lw = 1.5 ) if ylim_var : axhell . set_ylim ( ylim_var ) if xlim_var : axhell . set_xlim ( x_data [ 0 ], x_data [ - 1 ]) if col_var == \"RAND\" : axhell . plot ( x_data , y_data , ls = ls_var , lw = lw_var , alpha = alpha1_var , label = main_label ) else : axhell . plot ( x_data , y_data , ls = ls_var , color = col_var , lw = lw_var , alpha = alpha1_var , label = main_label , ) # print additional lines if present if kwargs : for key , value in kwargs . items (): aux_plot = { \"xdat\" : None , \"ydat\" : None , \"ls\" : \"-\" , \"col\" : \"k\" , \"lw\" : 1.5 , \"alph\" : 1.0 , \"aux_lab\" : None , \"mode\" : \"line\" , } for subkey , subval in value . items (): aux_plot [ subkey ] = subval if aux_plot [ \"mode\" ] == \"line\" : axhell . plot ( aux_plot [ \"xdat\" ], aux_plot [ \"ydat\" ], ls = aux_plot [ \"ls\" ], color = aux_plot [ \"col\" ], lw = aux_plot [ \"lw\" ], alpha = aux_plot [ \"alph\" ], label = aux_plot [ \"aux_lab\" ], ) elif aux_plot [ \"mode\" ] == \"scatter\" : axhell . scatter ( aux_plot [ \"xdat\" ], aux_plot [ \"ydat\" ], marker = aux_plot [ \"ls\" ], color = aux_plot [ \"col\" ], s = aux_plot [ \"lw\" ], alpha = aux_plot [ \"alph\" ], ) else : raise Exception ( \"Extra line plotting mode not found\" ) if main_label : axhell . set_leg = axhell . legend ( loc = \"best\" , fancybox = True ) axhell . set_leg . get_frame () . set_alpha ( 0.5 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) return def list_duplicates ( seq ): \"\"\"Uses default dict to find index numbers of repeated elements in a sequence. \"\"\" tally = defaultdict ( list ) for idx , item in enumerate ( seq ): tally [ str ( item )] . append ( idx ) return tally def split_list_on_missing_elem ( in_list ): \"\"\" in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence \"\"\" idx_holder = [] for k , g in groupby ( enumerate ( in_list ), lambda i : i [ 0 ] - i [ 1 ]): subli = list ( map ( itemgetter ( 1 ), g )) idx_holder . append ( subli ) return idx_holder def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ): \"\"\"Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. \"\"\" # get indexes of anomalous points & corresponding y-axis valuesA anomalous_truth_mask = residuals [ \"trig_anom\" ] . tolist () anomalous_times = residuals . index . tolist () anomalous_nc_y = norm_counts [ col_date ] . tolist () anomalous_ncres_y = residuals [ col_date ] . tolist () # print(\"\\nanomalous_truth_mask:\", anomalous_truth_mask) # print(\"\\nanomalous_times:\", anomalous_times) # print(\"\\nanomalous_nc_y:\", anomalous_nc_y) # print(\"\\nanomalous_ncres_y:\", anomalous_ncres_y) # print(\"\\n\") # split anomalus lists into nested lists - to plot properly truth_idxes = list_duplicates ( anomalous_truth_mask ) split_idxes = split_list_on_missing_elem ( truth_idxes [ \"True\" ]) # plot figure figx = figure () ax1 = figx . add_subplot ( 2 , 1 , 1 ) ax1 . set_title ( \"Anomaly Standardised Counts\" ) ax1 . grid ( True ) ax1 . set_ylim ( - 1.75 , 1.75 ) ax1 . set_ylabel ( \"Norm counts\" ) ax1 . set_xlabel ( \"\" ) ax1 . set_xticklabels ([]) for sub_list in split_idxes : anom_time_sub = [ anomalous_times [ idx ] for idx in sub_list ] anom_ncy_sub = [ anomalous_nc_y [ idx ] for idx in sub_list ] ax1 . plot ( anom_time_sub , anom_ncy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects = [ pe . Stroke ( linewidth = 4.6 , foreground = \"black\" ), pe . Normal ()], alpha = 1.0 , ) ax1 . plot ( residuals [ col_date ] . index , norm_counts [ col_date ], color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects = [ pe . Stroke ( linewidth = 2.0 , foreground = \"black\" ), pe . Normal ()], label = \"Event\" , ) ax1 . plot ( residuals [ col_date ] . index , normc_median [ \"median_of_cols\" ], color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Median\" , ) ax1 . set_leg = ax1 . legend ( loc = \"best\" , fancybox = True ) ax1 . set_leg . get_frame () . set_alpha ( 0.5 ) ### ax2 = figx . add_subplot ( 2 , 1 , 2 ) ax2 . set_title ( \"Euclidian distances - myRig: \" + str ( myrigidity )) ax2 . grid ( True ) ax2 . set_ylim ( - 0.05 , 1.5 ) ax2 . set_ylabel ( \"Norm Abs counts\" ) ax2 . set_xlabel ( \"Time\" ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times [ idx ] for idx in sub_list ] anom_ncresy_sub = [ anomalous_ncres_y [ idx ] for idx in sub_list ] ax2 . plot ( anom_time_sub , anom_ncresy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects = [ pe . Stroke ( linewidth = 4.6 , foreground = \"black\" ), pe . Normal ()], alpha = 1.0 , ) ax2 . plot ( residuals [ col_date ] . index , residuals [ col_date ], color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects = [ pe . Stroke ( linewidth = 2.0 , foreground = \"black\" ), pe . Normal ()], label = \"Event residual\" , ) ax2 . plot ( residuals [ col_date ] . index , residual_mad [ \"mad_of_myrig\" ], color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Residual median\" , ) ax2 . set_leg = ax2 . legend ( loc = \"best\" , fancybox = True ) ax2 . set_leg . get_frame () . set_alpha ( 0.5 ) for myax in figx . axes : matplotlib . pyplot . sca ( myax ) plt . xticks ( rotation = 15 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( img_path + \"anomalies_\" + str ( col_date ) + \".png\" ) plt . cla () plt . close ( figx ) return def plot_err_bar_graphs ( graph_data_dic ): \"\"\"Plot bar graphs with error bars.\"\"\" y_vals_keys = [ \"median_li\" , \"mean_li\" ] for y_key in y_vals_keys : fig_name = y_key + \".png\" title_var = y_key y_vals = graph_data_dic [ y_key ] x_pos = list ( range ( 0 , len ( y_vals ), 1 )) print ( \" \\n CC\" , y_key , y_vals ) fig = figure () plt . xticks ( rotation = 90 ) plt . title ( title_var ) ax1 = plt . subplot ( 1 , 1 , 1 ) ax1 . grid ( True ) ax1 . set_title ( title_var ) # ax1.axhline(y=2.0, color=\"grey\", linestyle='dashed', lw=1.5) ax1 . bar ( x_pos , y_vals , width = 0.8 , color = \"g\" , ecolor = \"k\" , yerr = graph_data_dic [ \"stddev_li\" ], tick_label = graph_data_dic [ \"fn_li\" ], capsize = 0.2 , alpha = 0.55 , ) # ax1.plot(x_axis, value) # xxxxxxxxxxxxxx plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) # plt.show() plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) Functions concat_images def concat_images ( file_li , outname , mode = 'horiz' , del_input = True ) Pastes inages into horizontal rows or vertical columns. View Source def concat_images ( file_li , outname , mode = \"horiz\" , del_input = True ): \"\"\"Pastes inages into horizontal rows or vertical columns.\"\"\" if mode == \"horiz\" : append = \"+append\" elif mode == \"vert\" : append = \"-append\" formated_cmd = [ \"convert\" , append ] for file_name in file_li : formated_cmd . append ( file_name ) formated_cmd . append ( outname ) subprocess . call ( formated_cmd ) if del_input : for file_name in file_li : os . remove ( file_name ) return grouper def grouper ( mylist , subli_size ) Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx View Source def grouper ( mylist , subli_size ): \"\"\"Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\"\" args = [ iter ( mylist )] * subli_size return zip_longest ( * args , fillvalue = None ) image_pager def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ) Concats a list of images into an NxN page using imagemagick . View Source def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ): \"\"\"Concats a list of images into an NxN page using imagemagick .\"\"\" horiz_paste = grouper ( img_list , horiz_n ) vert_img_li = [] for idx , row in enumerate ( horiz_paste ): img_name_li = [ path_str + \"/\" + img_name for img_name in row if img_name is not None ] out_img_name = path_str + \"/row_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"horiz\" ) vert_img_li . append ( out_img_name ) ### vert_paste = grouper ( vert_img_li , vert_n ) for idx , col in enumerate ( vert_paste ): img_name_li = [ row for row in col if row is not None ] out_img_name = path_str + fin_prefix + \"_page_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"vert\" ) return None list_duplicates def list_duplicates ( seq ) Uses default dict to find index numbers of repeated elements in a sequence. View Source def list_duplicates ( seq ): \"\"\"Uses default dict to find index numbers of repeated elements in a sequence. \"\"\" tally = defaultdict ( list ) for idx , item in enumerate ( seq ): tally [ str ( item )]. append ( idx ) return tally nested_line_plot def nested_line_plot ( x_data = None , y_data = None , ls_var = '--' , col_var = 'RAND' , lw_var = 1.0 , alpha1_var = 0.5 , title_var = 'null' , fig_name = 'null.png' , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ) Multi-line function call. kwargs are dictionaries. View Source def nested_line_plot ( x_data = None , y_data = None , ls_var = \"--\" , col_var = \"RAND\" , lw_var = 1.0 , alpha1_var = 0.5 , title_var = \"null\" , fig_name = \"null.png\" , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ) : \"\"\" Multi-line function call. kwargs are dictionaries. \"\"\" fig = figure () plt . xticks ( rotation = 15 ) plt . title ( title_var ) axhell = plt . subplot ( 1 , 1 , 1 ) axhell . grid ( True ) axhell . set_title ( title_var ) axhell . axhline ( y = 2.0 , color = \"grey\" , linestyle = \"dashed\" , lw = 1.5 ) if ylim_var : axhell . set_ylim ( ylim_var ) if xlim_var : axhell . set_xlim ( x_data [ 0 ] , x_data [ -1 ] ) if col_var == \"RAND\" : axhell . plot ( x_data , y_data , ls = ls_var , lw = lw_var , alpha = alpha1_var , label = main_label ) else : axhell . plot ( x_data , y_data , ls = ls_var , color = col_var , lw = lw_var , alpha = alpha1_var , label = main_label , ) # print additional lines if present if kwargs : for key , value in kwargs . items () : aux_plot = { \"xdat\" : None , \"ydat\" : None , \"ls\" : \"-\" , \"col\" : \"k\" , \"lw\" : 1.5 , \"alph\" : 1.0 , \"aux_lab\" : None , \"mode\" : \"line\" , } for subkey , subval in value . items () : aux_plot [ subkey ] = subval if aux_plot [ \"mode\" ] == \"line\" : axhell . plot ( aux_plot [ \"xdat\" ] , aux_plot [ \"ydat\" ] , ls = aux_plot [ \"ls\" ] , color = aux_plot [ \"col\" ] , lw = aux_plot [ \"lw\" ] , alpha = aux_plot [ \"alph\" ] , label = aux_plot [ \"aux_lab\" ] , ) elif aux_plot [ \"mode\" ] == \"scatter\" : axhell . scatter ( aux_plot [ \"xdat\" ] , aux_plot [ \"ydat\" ] , marker = aux_plot [ \"ls\" ] , color = aux_plot [ \"col\" ] , s = aux_plot [ \"lw\" ] , alpha = aux_plot [ \"alph\" ] , ) else : raise Exception ( \"Extra line plotting mode not found\" ) if main_label : axhell . set_leg = axhell . legend ( loc = \"best\" , fancybox = True ) axhell . set_leg . get_frame (). set_alpha ( 0.5 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) return plot_err_bar_graphs def plot_err_bar_graphs ( graph_data_dic ) Plot bar graphs with error bars. View Source def plot_err_bar_graphs ( graph_data_dic ) : \"\"\"Plot bar graphs with error bars.\"\"\" y_vals_keys = [ \"median_li\", \"mean_li\" ] for y_key in y_vals_keys : fig_name = y_key + \".png\" title_var = y_key y_vals = graph_data_dic [ y_key ] x_pos = list ( range ( 0 , len ( y_vals ), 1 )) print ( \"\\nCC\" , y_key , y_vals ) fig = figure () plt . xticks ( rotation = 90 ) plt . title ( title_var ) ax1 = plt . subplot ( 1 , 1 , 1 ) ax1 . grid ( True ) ax1 . set_title ( title_var ) # ax1 . axhline ( y = 2.0 , color = \"grey\" , linestyle = 'dashed' , lw = 1.5 ) ax1 . bar ( x_pos , y_vals , width = 0.8 , color = \"g\" , ecolor = \"k\" , yerr = graph_data_dic [ \"stddev_li\" ] , tick_label = graph_data_dic [ \"fn_li\" ] , capsize = 0.2 , alpha = 0.55 , ) # ax1 . plot ( x_axis , value ) # xxxxxxxxxxxxxx plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) # plt . show () plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) split_list_on_missing_elem def split_list_on_missing_elem ( in_list ) in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence View Source def split_list_on_missing_elem ( in_list ): \"\"\" in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence \"\"\" idx_holder = [] for k , g in groupby ( enumerate ( in_list ), lambda i : i [ 0 ] - i [ 1 ]): subli = list ( map ( itemgetter ( 1 ), g )) idx_holder . append ( subli ) return idx_holder viz_anamoly_plot def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ) Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. View Source def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ) : \"\"\"Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. \"\"\" # get indexes of anomalous points & corresponding y - axis valuesA anomalous_truth_mask = residuals [ \"trig_anom\" ] . tolist () anomalous_times = residuals . index . tolist () anomalous_nc_y = norm_counts [ col_date ] . tolist () anomalous_ncres_y = residuals [ col_date ] . tolist () # print ( \"\\nanomalous_truth_mask:\" , anomalous_truth_mask ) # print ( \"\\nanomalous_times:\" , anomalous_times ) # print ( \"\\nanomalous_nc_y:\" , anomalous_nc_y ) # print ( \"\\nanomalous_ncres_y:\" , anomalous_ncres_y ) # print ( \"\\n\" ) # split anomalus lists into nested lists - to plot properly truth_idxes = list_duplicates ( anomalous_truth_mask ) split_idxes = split_list_on_missing_elem ( truth_idxes [ \"True\" ] ) # plot figure figx = figure () ax1 = figx . add_subplot ( 2 , 1 , 1 ) ax1 . set_title ( \"Anomaly Standardised Counts\" ) ax1 . grid ( True ) ax1 . set_ylim ( - 1.75 , 1.75 ) ax1 . set_ylabel ( \"Norm counts\" ) ax1 . set_xlabel ( \"\" ) ax1 . set_xticklabels ( [] ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times[idx ] for idx in sub_list ] anom_ncy_sub = [ anomalous_nc_y[idx ] for idx in sub_list ] ax1 . plot ( anom_time_sub , anom_ncy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects =[ pe.Stroke(linewidth=4.6, foreground=\"black\"), pe.Normal() ] , alpha = 1.0 , ) ax1 . plot ( residuals [ col_date ] . index , norm_counts [ col_date ] , color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects =[ pe.Stroke(linewidth=2.0, foreground=\"black\"), pe.Normal() ] , label = \"Event\" , ) ax1 . plot ( residuals [ col_date ] . index , normc_median [ \"median_of_cols\" ] , color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Median\" , ) ax1 . set_leg = ax1 . legend ( loc = \"best\" , fancybox = True ) ax1 . set_leg . get_frame (). set_alpha ( 0.5 ) ### ax2 = figx . add_subplot ( 2 , 1 , 2 ) ax2 . set_title ( \"Euclidian distances - myRig: \" + str ( myrigidity )) ax2 . grid ( True ) ax2 . set_ylim ( - 0.05 , 1.5 ) ax2 . set_ylabel ( \"Norm Abs counts\" ) ax2 . set_xlabel ( \"Time\" ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times[idx ] for idx in sub_list ] anom_ncresy_sub = [ anomalous_ncres_y[idx ] for idx in sub_list ] ax2 . plot ( anom_time_sub , anom_ncresy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects =[ pe.Stroke(linewidth=4.6, foreground=\"black\"), pe.Normal() ] , alpha = 1.0 , ) ax2 . plot ( residuals [ col_date ] . index , residuals [ col_date ] , color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects =[ pe.Stroke(linewidth=2.0, foreground=\"black\"), pe.Normal() ] , label = \"Event residual\" , ) ax2 . plot ( residuals [ col_date ] . index , residual_mad [ \"mad_of_myrig\" ] , color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Residual median\" , ) ax2 . set_leg = ax2 . legend ( loc = \"best\" , fancybox = True ) ax2 . set_leg . get_frame (). set_alpha ( 0.5 ) for myax in figx . axes : matplotlib . pyplot . sca ( myax ) plt . xticks ( rotation = 15 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( img_path + \"anomalies_\" + str ( col_date ) + \".png\" ) plt . cla () plt . close ( figx ) return","title":"Plotter Tools"},{"location":"reference/helpers/plotter_tools/#module-helpersplotter_tools","text":"Functions to assist in plotting images. View Source \"\"\"Functions to assist in plotting images.\"\"\" import os as os import subprocess from collections import defaultdict from operator import itemgetter from itertools import zip_longest , groupby import matplotlib matplotlib . use ( \"agg\" ) import matplotlib.pyplot as plt import matplotlib.patheffects as pe from matplotlib import rcParams from pylab import figure rcParams [ \"axes.labelsize\" ] = 14 rcParams [ \"xtick.labelsize\" ] = 14 rcParams [ \"ytick.labelsize\" ] = 14 rcParams [ \"legend.fontsize\" ] = 12 rcParams [ \"font.family\" ] = \"Dejavu Sans\" rcParams [ \"font.serif\" ] = [ \"Computer Modern Roman\" ] rcParams [ \"xtick.major.pad\" ] = 12 rcParams [ \"ytick.major.pad\" ] = 12 def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ): \"\"\"Concats a list of images into an NxN page using imagemagick .\"\"\" horiz_paste = grouper ( img_list , horiz_n ) vert_img_li = [] for idx , row in enumerate ( horiz_paste ): img_name_li = [ path_str + \"/\" + img_name for img_name in row if img_name is not None ] out_img_name = path_str + \"/row_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"horiz\" ) vert_img_li . append ( out_img_name ) ### vert_paste = grouper ( vert_img_li , vert_n ) for idx , col in enumerate ( vert_paste ): img_name_li = [ row for row in col if row is not None ] out_img_name = path_str + fin_prefix + \"_page_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"vert\" ) return None def grouper ( mylist , subli_size ): \"\"\"Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\"\" args = [ iter ( mylist )] * subli_size return zip_longest ( * args , fillvalue = None ) def concat_images ( file_li , outname , mode = \"horiz\" , del_input = True ): \"\"\"Pastes inages into horizontal rows or vertical columns.\"\"\" if mode == \"horiz\" : append = \"+append\" elif mode == \"vert\" : append = \"-append\" formated_cmd = [ \"convert\" , append ] for file_name in file_li : formated_cmd . append ( file_name ) formated_cmd . append ( outname ) subprocess . call ( formated_cmd ) if del_input : for file_name in file_li : os . remove ( file_name ) return def nested_line_plot ( x_data = None , y_data = None , ls_var = \"--\" , col_var = \"RAND\" , lw_var = 1.0 , alpha1_var = 0.5 , title_var = \"null\" , fig_name = \"null.png\" , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ): \"\"\" Multi-line function call. kwargs are dictionaries. \"\"\" fig = figure () plt . xticks ( rotation = 15 ) plt . title ( title_var ) axhell = plt . subplot ( 1 , 1 , 1 ) axhell . grid ( True ) axhell . set_title ( title_var ) axhell . axhline ( y = 2.0 , color = \"grey\" , linestyle = \"dashed\" , lw = 1.5 ) if ylim_var : axhell . set_ylim ( ylim_var ) if xlim_var : axhell . set_xlim ( x_data [ 0 ], x_data [ - 1 ]) if col_var == \"RAND\" : axhell . plot ( x_data , y_data , ls = ls_var , lw = lw_var , alpha = alpha1_var , label = main_label ) else : axhell . plot ( x_data , y_data , ls = ls_var , color = col_var , lw = lw_var , alpha = alpha1_var , label = main_label , ) # print additional lines if present if kwargs : for key , value in kwargs . items (): aux_plot = { \"xdat\" : None , \"ydat\" : None , \"ls\" : \"-\" , \"col\" : \"k\" , \"lw\" : 1.5 , \"alph\" : 1.0 , \"aux_lab\" : None , \"mode\" : \"line\" , } for subkey , subval in value . items (): aux_plot [ subkey ] = subval if aux_plot [ \"mode\" ] == \"line\" : axhell . plot ( aux_plot [ \"xdat\" ], aux_plot [ \"ydat\" ], ls = aux_plot [ \"ls\" ], color = aux_plot [ \"col\" ], lw = aux_plot [ \"lw\" ], alpha = aux_plot [ \"alph\" ], label = aux_plot [ \"aux_lab\" ], ) elif aux_plot [ \"mode\" ] == \"scatter\" : axhell . scatter ( aux_plot [ \"xdat\" ], aux_plot [ \"ydat\" ], marker = aux_plot [ \"ls\" ], color = aux_plot [ \"col\" ], s = aux_plot [ \"lw\" ], alpha = aux_plot [ \"alph\" ], ) else : raise Exception ( \"Extra line plotting mode not found\" ) if main_label : axhell . set_leg = axhell . legend ( loc = \"best\" , fancybox = True ) axhell . set_leg . get_frame () . set_alpha ( 0.5 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) return def list_duplicates ( seq ): \"\"\"Uses default dict to find index numbers of repeated elements in a sequence. \"\"\" tally = defaultdict ( list ) for idx , item in enumerate ( seq ): tally [ str ( item )] . append ( idx ) return tally def split_list_on_missing_elem ( in_list ): \"\"\" in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence \"\"\" idx_holder = [] for k , g in groupby ( enumerate ( in_list ), lambda i : i [ 0 ] - i [ 1 ]): subli = list ( map ( itemgetter ( 1 ), g )) idx_holder . append ( subli ) return idx_holder def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ): \"\"\"Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. \"\"\" # get indexes of anomalous points & corresponding y-axis valuesA anomalous_truth_mask = residuals [ \"trig_anom\" ] . tolist () anomalous_times = residuals . index . tolist () anomalous_nc_y = norm_counts [ col_date ] . tolist () anomalous_ncres_y = residuals [ col_date ] . tolist () # print(\"\\nanomalous_truth_mask:\", anomalous_truth_mask) # print(\"\\nanomalous_times:\", anomalous_times) # print(\"\\nanomalous_nc_y:\", anomalous_nc_y) # print(\"\\nanomalous_ncres_y:\", anomalous_ncres_y) # print(\"\\n\") # split anomalus lists into nested lists - to plot properly truth_idxes = list_duplicates ( anomalous_truth_mask ) split_idxes = split_list_on_missing_elem ( truth_idxes [ \"True\" ]) # plot figure figx = figure () ax1 = figx . add_subplot ( 2 , 1 , 1 ) ax1 . set_title ( \"Anomaly Standardised Counts\" ) ax1 . grid ( True ) ax1 . set_ylim ( - 1.75 , 1.75 ) ax1 . set_ylabel ( \"Norm counts\" ) ax1 . set_xlabel ( \"\" ) ax1 . set_xticklabels ([]) for sub_list in split_idxes : anom_time_sub = [ anomalous_times [ idx ] for idx in sub_list ] anom_ncy_sub = [ anomalous_nc_y [ idx ] for idx in sub_list ] ax1 . plot ( anom_time_sub , anom_ncy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects = [ pe . Stroke ( linewidth = 4.6 , foreground = \"black\" ), pe . Normal ()], alpha = 1.0 , ) ax1 . plot ( residuals [ col_date ] . index , norm_counts [ col_date ], color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects = [ pe . Stroke ( linewidth = 2.0 , foreground = \"black\" ), pe . Normal ()], label = \"Event\" , ) ax1 . plot ( residuals [ col_date ] . index , normc_median [ \"median_of_cols\" ], color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Median\" , ) ax1 . set_leg = ax1 . legend ( loc = \"best\" , fancybox = True ) ax1 . set_leg . get_frame () . set_alpha ( 0.5 ) ### ax2 = figx . add_subplot ( 2 , 1 , 2 ) ax2 . set_title ( \"Euclidian distances - myRig: \" + str ( myrigidity )) ax2 . grid ( True ) ax2 . set_ylim ( - 0.05 , 1.5 ) ax2 . set_ylabel ( \"Norm Abs counts\" ) ax2 . set_xlabel ( \"Time\" ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times [ idx ] for idx in sub_list ] anom_ncresy_sub = [ anomalous_ncres_y [ idx ] for idx in sub_list ] ax2 . plot ( anom_time_sub , anom_ncresy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects = [ pe . Stroke ( linewidth = 4.6 , foreground = \"black\" ), pe . Normal ()], alpha = 1.0 , ) ax2 . plot ( residuals [ col_date ] . index , residuals [ col_date ], color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects = [ pe . Stroke ( linewidth = 2.0 , foreground = \"black\" ), pe . Normal ()], label = \"Event residual\" , ) ax2 . plot ( residuals [ col_date ] . index , residual_mad [ \"mad_of_myrig\" ], color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Residual median\" , ) ax2 . set_leg = ax2 . legend ( loc = \"best\" , fancybox = True ) ax2 . set_leg . get_frame () . set_alpha ( 0.5 ) for myax in figx . axes : matplotlib . pyplot . sca ( myax ) plt . xticks ( rotation = 15 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( img_path + \"anomalies_\" + str ( col_date ) + \".png\" ) plt . cla () plt . close ( figx ) return def plot_err_bar_graphs ( graph_data_dic ): \"\"\"Plot bar graphs with error bars.\"\"\" y_vals_keys = [ \"median_li\" , \"mean_li\" ] for y_key in y_vals_keys : fig_name = y_key + \".png\" title_var = y_key y_vals = graph_data_dic [ y_key ] x_pos = list ( range ( 0 , len ( y_vals ), 1 )) print ( \" \\n CC\" , y_key , y_vals ) fig = figure () plt . xticks ( rotation = 90 ) plt . title ( title_var ) ax1 = plt . subplot ( 1 , 1 , 1 ) ax1 . grid ( True ) ax1 . set_title ( title_var ) # ax1.axhline(y=2.0, color=\"grey\", linestyle='dashed', lw=1.5) ax1 . bar ( x_pos , y_vals , width = 0.8 , color = \"g\" , ecolor = \"k\" , yerr = graph_data_dic [ \"stddev_li\" ], tick_label = graph_data_dic [ \"fn_li\" ], capsize = 0.2 , alpha = 0.55 , ) # ax1.plot(x_axis, value) # xxxxxxxxxxxxxx plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) # plt.show() plt . savefig ( fig_name ) plt . cla () plt . close ( fig )","title":"Module helpers.plotter_tools"},{"location":"reference/helpers/plotter_tools/#functions","text":"","title":"Functions"},{"location":"reference/helpers/plotter_tools/#concat_images","text":"def concat_images ( file_li , outname , mode = 'horiz' , del_input = True ) Pastes inages into horizontal rows or vertical columns. View Source def concat_images ( file_li , outname , mode = \"horiz\" , del_input = True ): \"\"\"Pastes inages into horizontal rows or vertical columns.\"\"\" if mode == \"horiz\" : append = \"+append\" elif mode == \"vert\" : append = \"-append\" formated_cmd = [ \"convert\" , append ] for file_name in file_li : formated_cmd . append ( file_name ) formated_cmd . append ( outname ) subprocess . call ( formated_cmd ) if del_input : for file_name in file_li : os . remove ( file_name ) return","title":"concat_images"},{"location":"reference/helpers/plotter_tools/#grouper","text":"def grouper ( mylist , subli_size ) Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx View Source def grouper ( mylist , subli_size ): \"\"\"Collect data into fixed-length chunks or blocks grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\"\" args = [ iter ( mylist )] * subli_size return zip_longest ( * args , fillvalue = None )","title":"grouper"},{"location":"reference/helpers/plotter_tools/#image_pager","text":"def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ) Concats a list of images into an NxN page using imagemagick . View Source def image_pager ( img_list , horiz_n , vert_n , path_str , fin_prefix ): \"\"\"Concats a list of images into an NxN page using imagemagick .\"\"\" horiz_paste = grouper ( img_list , horiz_n ) vert_img_li = [] for idx , row in enumerate ( horiz_paste ): img_name_li = [ path_str + \"/\" + img_name for img_name in row if img_name is not None ] out_img_name = path_str + \"/row_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"horiz\" ) vert_img_li . append ( out_img_name ) ### vert_paste = grouper ( vert_img_li , vert_n ) for idx , col in enumerate ( vert_paste ): img_name_li = [ row for row in col if row is not None ] out_img_name = path_str + fin_prefix + \"_page_\" + str ( idx ) + \".png\" concat_images ( file_li = img_name_li , outname = out_img_name , mode = \"vert\" ) return None","title":"image_pager"},{"location":"reference/helpers/plotter_tools/#list_duplicates","text":"def list_duplicates ( seq ) Uses default dict to find index numbers of repeated elements in a sequence. View Source def list_duplicates ( seq ): \"\"\"Uses default dict to find index numbers of repeated elements in a sequence. \"\"\" tally = defaultdict ( list ) for idx , item in enumerate ( seq ): tally [ str ( item )]. append ( idx ) return tally","title":"list_duplicates"},{"location":"reference/helpers/plotter_tools/#nested_line_plot","text":"def nested_line_plot ( x_data = None , y_data = None , ls_var = '--' , col_var = 'RAND' , lw_var = 1.0 , alpha1_var = 0.5 , title_var = 'null' , fig_name = 'null.png' , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ) Multi-line function call. kwargs are dictionaries. View Source def nested_line_plot ( x_data = None , y_data = None , ls_var = \"--\" , col_var = \"RAND\" , lw_var = 1.0 , alpha1_var = 0.5 , title_var = \"null\" , fig_name = \"null.png\" , ylim_var = False , xlim_var = False , main_label = None , ** kwargs ) : \"\"\" Multi-line function call. kwargs are dictionaries. \"\"\" fig = figure () plt . xticks ( rotation = 15 ) plt . title ( title_var ) axhell = plt . subplot ( 1 , 1 , 1 ) axhell . grid ( True ) axhell . set_title ( title_var ) axhell . axhline ( y = 2.0 , color = \"grey\" , linestyle = \"dashed\" , lw = 1.5 ) if ylim_var : axhell . set_ylim ( ylim_var ) if xlim_var : axhell . set_xlim ( x_data [ 0 ] , x_data [ -1 ] ) if col_var == \"RAND\" : axhell . plot ( x_data , y_data , ls = ls_var , lw = lw_var , alpha = alpha1_var , label = main_label ) else : axhell . plot ( x_data , y_data , ls = ls_var , color = col_var , lw = lw_var , alpha = alpha1_var , label = main_label , ) # print additional lines if present if kwargs : for key , value in kwargs . items () : aux_plot = { \"xdat\" : None , \"ydat\" : None , \"ls\" : \"-\" , \"col\" : \"k\" , \"lw\" : 1.5 , \"alph\" : 1.0 , \"aux_lab\" : None , \"mode\" : \"line\" , } for subkey , subval in value . items () : aux_plot [ subkey ] = subval if aux_plot [ \"mode\" ] == \"line\" : axhell . plot ( aux_plot [ \"xdat\" ] , aux_plot [ \"ydat\" ] , ls = aux_plot [ \"ls\" ] , color = aux_plot [ \"col\" ] , lw = aux_plot [ \"lw\" ] , alpha = aux_plot [ \"alph\" ] , label = aux_plot [ \"aux_lab\" ] , ) elif aux_plot [ \"mode\" ] == \"scatter\" : axhell . scatter ( aux_plot [ \"xdat\" ] , aux_plot [ \"ydat\" ] , marker = aux_plot [ \"ls\" ] , color = aux_plot [ \"col\" ] , s = aux_plot [ \"lw\" ] , alpha = aux_plot [ \"alph\" ] , ) else : raise Exception ( \"Extra line plotting mode not found\" ) if main_label : axhell . set_leg = axhell . legend ( loc = \"best\" , fancybox = True ) axhell . set_leg . get_frame (). set_alpha ( 0.5 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( fig_name ) plt . cla () plt . close ( fig ) return","title":"nested_line_plot"},{"location":"reference/helpers/plotter_tools/#plot_err_bar_graphs","text":"def plot_err_bar_graphs ( graph_data_dic ) Plot bar graphs with error bars. View Source def plot_err_bar_graphs ( graph_data_dic ) : \"\"\"Plot bar graphs with error bars.\"\"\" y_vals_keys = [ \"median_li\", \"mean_li\" ] for y_key in y_vals_keys : fig_name = y_key + \".png\" title_var = y_key y_vals = graph_data_dic [ y_key ] x_pos = list ( range ( 0 , len ( y_vals ), 1 )) print ( \"\\nCC\" , y_key , y_vals ) fig = figure () plt . xticks ( rotation = 90 ) plt . title ( title_var ) ax1 = plt . subplot ( 1 , 1 , 1 ) ax1 . grid ( True ) ax1 . set_title ( title_var ) # ax1 . axhline ( y = 2.0 , color = \"grey\" , linestyle = 'dashed' , lw = 1.5 ) ax1 . bar ( x_pos , y_vals , width = 0.8 , color = \"g\" , ecolor = \"k\" , yerr = graph_data_dic [ \"stddev_li\" ] , tick_label = graph_data_dic [ \"fn_li\" ] , capsize = 0.2 , alpha = 0.55 , ) # ax1 . plot ( x_axis , value ) # xxxxxxxxxxxxxx plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) # plt . show () plt . savefig ( fig_name ) plt . cla () plt . close ( fig )","title":"plot_err_bar_graphs"},{"location":"reference/helpers/plotter_tools/#split_list_on_missing_elem","text":"def split_list_on_missing_elem ( in_list ) in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence View Source def split_list_on_missing_elem ( in_list ): \"\"\" in: [1, 2, 3, 4, 6, 7, 8, 9, 10] out: [[1, 2, 3, 4], [6, 7, 8, 9, 10]] https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence \"\"\" idx_holder = [] for k , g in groupby ( enumerate ( in_list ), lambda i : i [ 0 ] - i [ 1 ]): subli = list ( map ( itemgetter ( 1 ), g )) idx_holder . append ( subli ) return idx_holder","title":"split_list_on_missing_elem"},{"location":"reference/helpers/plotter_tools/#viz_anamoly_plot","text":"def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ) Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. View Source def viz_anamoly_plot ( col_date , residuals , residual_mad , norm_counts , normc_median , img_path , myrigidity ) : \"\"\"Visualise the identified anomaly. Plots normalised counts and euclidian distances with anomaly highlighted in red. \"\"\" # get indexes of anomalous points & corresponding y - axis valuesA anomalous_truth_mask = residuals [ \"trig_anom\" ] . tolist () anomalous_times = residuals . index . tolist () anomalous_nc_y = norm_counts [ col_date ] . tolist () anomalous_ncres_y = residuals [ col_date ] . tolist () # print ( \"\\nanomalous_truth_mask:\" , anomalous_truth_mask ) # print ( \"\\nanomalous_times:\" , anomalous_times ) # print ( \"\\nanomalous_nc_y:\" , anomalous_nc_y ) # print ( \"\\nanomalous_ncres_y:\" , anomalous_ncres_y ) # print ( \"\\n\" ) # split anomalus lists into nested lists - to plot properly truth_idxes = list_duplicates ( anomalous_truth_mask ) split_idxes = split_list_on_missing_elem ( truth_idxes [ \"True\" ] ) # plot figure figx = figure () ax1 = figx . add_subplot ( 2 , 1 , 1 ) ax1 . set_title ( \"Anomaly Standardised Counts\" ) ax1 . grid ( True ) ax1 . set_ylim ( - 1.75 , 1.75 ) ax1 . set_ylabel ( \"Norm counts\" ) ax1 . set_xlabel ( \"\" ) ax1 . set_xticklabels ( [] ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times[idx ] for idx in sub_list ] anom_ncy_sub = [ anomalous_nc_y[idx ] for idx in sub_list ] ax1 . plot ( anom_time_sub , anom_ncy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects =[ pe.Stroke(linewidth=4.6, foreground=\"black\"), pe.Normal() ] , alpha = 1.0 , ) ax1 . plot ( residuals [ col_date ] . index , norm_counts [ col_date ] , color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects =[ pe.Stroke(linewidth=2.0, foreground=\"black\"), pe.Normal() ] , label = \"Event\" , ) ax1 . plot ( residuals [ col_date ] . index , normc_median [ \"median_of_cols\" ] , color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Median\" , ) ax1 . set_leg = ax1 . legend ( loc = \"best\" , fancybox = True ) ax1 . set_leg . get_frame (). set_alpha ( 0.5 ) ### ax2 = figx . add_subplot ( 2 , 1 , 2 ) ax2 . set_title ( \"Euclidian distances - myRig: \" + str ( myrigidity )) ax2 . grid ( True ) ax2 . set_ylim ( - 0.05 , 1.5 ) ax2 . set_ylabel ( \"Norm Abs counts\" ) ax2 . set_xlabel ( \"Time\" ) for sub_list in split_idxes : anom_time_sub = [ anomalous_times[idx ] for idx in sub_list ] anom_ncresy_sub = [ anomalous_ncres_y[idx ] for idx in sub_list ] ax2 . plot ( anom_time_sub , anom_ncresy_sub , color = \"orangered\" , ls = \"-\" , lw = 4.4 , path_effects =[ pe.Stroke(linewidth=4.6, foreground=\"black\"), pe.Normal() ] , alpha = 1.0 , ) ax2 . plot ( residuals [ col_date ] . index , residuals [ col_date ] , color = \"chartreuse\" , ls = \"-\" , lw = 1.5 , path_effects =[ pe.Stroke(linewidth=2.0, foreground=\"black\"), pe.Normal() ] , label = \"Event residual\" , ) ax2 . plot ( residuals [ col_date ] . index , residual_mad [ \"mad_of_myrig\" ] , color = \"dodgerblue\" , ls = \"--\" , lw = 1.5 , label = \"Residual median\" , ) ax2 . set_leg = ax2 . legend ( loc = \"best\" , fancybox = True ) ax2 . set_leg . get_frame (). set_alpha ( 0.5 ) for myax in figx . axes : matplotlib . pyplot . sca ( myax ) plt . xticks ( rotation = 15 ) plt . tight_layout ( pad = 0.9 , w_pad = 0.5 , h_pad = 1.0 ) plt . savefig ( img_path + \"anomalies_\" + str ( col_date ) + \".png\" ) plt . cla () plt . close ( figx ) return","title":"viz_anamoly_plot"},{"location":"reference/helpers/string_tools/","text":"Module helpers.string_tools View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey def stringify_values ( inlist ): \"\"\"Converts all items in dictionary key into strings.\"\"\" string_items = [] for item in inlist : if isinstance ( item , dict ): new_item = \" \" . join ( \"{} {},\" . format ( key , val ) for key , val in item . items ()) string_items . append ( new_item ) elif isinstance ( item , int ): new_item = str ( item ) string_items . append ( new_item ) elif isinstance ( item , str ): string_items . append ( item ) else : LOG . error ( \"failed stringify item is %s, type: %s\" , item , type ( item )) raise Exception ( \"failed stringify item is \" , item , type ( item )) return string_items Functions count_str_whitespace def count_str_whitespace ( mystr ) Counts no of whitespace in string and returns an int. View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count stringify_values def stringify_values ( inlist ) Converts all items in dictionary key into strings. View Source def stringify_values ( inlist ): \"\"\"Converts all items in dictionary key into strings.\"\"\" string_items = [] for item in inlist : if isinstance ( item , dict ): new_item = \" \" . join ( \"{} {},\" . format ( key , val ) for key , val in item . items ()) string_items . append ( new_item ) elif isinstance ( item , int ): new_item = str ( item ) string_items . append ( new_item ) elif isinstance ( item , str ): string_items . append ( item ) else : LOG . error ( \"failed stringify item is %s, type: %s\" , item , type ( item )) raise Exception ( \"failed stringify item is \" , item , type ( item )) return string_items unify_dic_key def unify_dic_key ( key ) View Source def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey","title":"String Tools"},{"location":"reference/helpers/string_tools/#module-helpersstring_tools","text":"View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey def stringify_values ( inlist ): \"\"\"Converts all items in dictionary key into strings.\"\"\" string_items = [] for item in inlist : if isinstance ( item , dict ): new_item = \" \" . join ( \"{} {},\" . format ( key , val ) for key , val in item . items ()) string_items . append ( new_item ) elif isinstance ( item , int ): new_item = str ( item ) string_items . append ( new_item ) elif isinstance ( item , str ): string_items . append ( item ) else : LOG . error ( \"failed stringify item is %s, type: %s\" , item , type ( item )) raise Exception ( \"failed stringify item is \" , item , type ( item )) return string_items","title":"Module helpers.string_tools"},{"location":"reference/helpers/string_tools/#functions","text":"","title":"Functions"},{"location":"reference/helpers/string_tools/#count_str_whitespace","text":"def count_str_whitespace ( mystr ) Counts no of whitespace in string and returns an int. View Source def count_str_whitespace ( mystr ): \"\"\"Counts no of whitespace in string and returns an int.\"\"\" count = 0 for mychar in mystr : if mychar . isspace (): count += 1 return count","title":"count_str_whitespace"},{"location":"reference/helpers/string_tools/#stringify_values","text":"def stringify_values ( inlist ) Converts all items in dictionary key into strings. View Source def stringify_values ( inlist ): \"\"\"Converts all items in dictionary key into strings.\"\"\" string_items = [] for item in inlist : if isinstance ( item , dict ): new_item = \" \" . join ( \"{} {},\" . format ( key , val ) for key , val in item . items ()) string_items . append ( new_item ) elif isinstance ( item , int ): new_item = str ( item ) string_items . append ( new_item ) elif isinstance ( item , str ): string_items . append ( item ) else : LOG . error ( \"failed stringify item is %s, type: %s\" , item , type ( item )) raise Exception ( \"failed stringify item is \" , item , type ( item )) return string_items","title":"stringify_values"},{"location":"reference/helpers/string_tools/#unify_dic_key","text":"def unify_dic_key ( key ) View Source def unify_dic_key ( key ): unifiedkey = key . replace ( \"_\" , \"\" ). replace ( \"-\" , \"\" ). lower () return unifiedkey","title":"unify_dic_key"}]}